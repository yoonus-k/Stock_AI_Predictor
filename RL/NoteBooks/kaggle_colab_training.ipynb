{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbff339a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Trading Model Training on Cloud\n",
    "\n",
    "This notebook allows you to train your RL trading model on cloud platforms like Google Colab or Kaggle. It handles all the necessary setup, including:\n",
    "\n",
    "1. Installing required packages\n",
    "2. Importing your codebase from GitHub\n",
    "3. Setting up the environment\n",
    "4. Training the model\n",
    "5. Saving the trained model\n",
    "6. Monitoring training progress\n",
    "\n",
    "## Why Use Cloud Platforms?\n",
    "\n",
    "Training reinforcement learning models can be very resource-intensive. Cloud platforms provide:\n",
    "- Access to GPUs for faster training\n",
    "- More memory resources\n",
    "- Persistent storage options\n",
    "- Pre-installed ML libraries\n",
    "\n",
    "Choose your platform:\n",
    "- **Google Colab**: Free GPUs with time limits (12hr sessions)\n",
    "- **Kaggle**: Free GPUs with more stability (30+ GPU hours weekly)\n",
    "- **Hugging Face**: Good for deployment and sharing your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b2208",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "First, let's install all the necessary packages. We'll use stable versions that are known to work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check if we're running on Colab\n",
    "import os\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "\n",
    "print(f\"Running on Colab: {IN_COLAB}\")\n",
    "print(f\"Running on Kaggle: {IN_KAGGLE}\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q stable-baselines3==2.0.0\n",
    "!pip install -q gymnasium\n",
    "!pip install -q pandas matplotlib seaborn tqdm\n",
    "!pip install -q tensorboard\n",
    "\n",
    "# Install optional packages for visualization\n",
    "!pip install -q plotly ipywidgets\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cf151",
   "metadata": {},
   "source": [
    "## Step 2: Clone or Update Repository\n",
    "\n",
    "Next, let's get your code. We'll either clone the repository or update it if it already exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb270b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set repository path\n",
    "REPO_PATH = \"Stock_AI_Predictor\"\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    !git clone https://github.com/yoonus-k/Stock_AI_Predictor.git\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    # If it exists, pull the latest changes\n",
    "    %cd $REPO_PATH\n",
    "    !git pull\n",
    "    %cd ..\n",
    "    print(\"Repository updated successfully!\")\n",
    "\n",
    "# Add the repository to Python path\n",
    "import sys\n",
    "if REPO_PATH not in sys.path:\n",
    "    sys.path.append(REPO_PATH)\n",
    "\n",
    "print(f\"Repository path: {os.path.abspath(REPO_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492bf12",
   "metadata": {},
   "source": [
    "## Step 3: Test Imports\n",
    "\n",
    "Let's make sure we can import all the required modules without errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cddee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try importing key modules to verify installation\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "    \n",
    "    print(\"All dependencies imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing dependencies: {e}\")\n",
    "    print(\"Please make sure all required packages are installed.\")\n",
    "    \n",
    "# Now try importing from our project structure\n",
    "try:\n",
    "    # Import specific modules from your project\n",
    "    from RL.Envs.trading_env import TradingEnv\n",
    "    from RL.Envs.action_wrapper import TupleActionWrapper\n",
    "    from RL.Data.loader import load_data_from_db\n",
    "    \n",
    "    print(\"Project modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Please ensure the repository structure is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955e4cc",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Data\n",
    "\n",
    "Let's set up the data for training. We need to:\n",
    "1. Create a database or upload existing data\n",
    "2. Prepare the data for the RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths based on environment\n",
    "if IN_COLAB or IN_KAGGLE:\n",
    "    # Cloud storage paths\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        # Mount Google Drive for persistent storage on Colab\n",
    "        drive.mount('/content/drive')\n",
    "        DATA_DIR = \"/content/drive/MyDrive/Stock_AI_Predictor/Data\"\n",
    "        MODEL_DIR = \"/content/drive/MyDrive/Stock_AI_Predictor/Models\"\n",
    "        LOG_DIR = \"/content/drive/MyDrive/Stock_AI_Predictor/Logs\"\n",
    "    else:  # Kaggle\n",
    "        DATA_DIR = \"/kaggle/working/data\"\n",
    "        MODEL_DIR = \"/kaggle/working/models\"\n",
    "        LOG_DIR = \"/kaggle/working/logs\"\n",
    "else:\n",
    "    # Local paths (for testing)\n",
    "    DATA_DIR = \"RL/Data\"\n",
    "    MODEL_DIR = \"RL/Models\"\n",
    "    LOG_DIR = \"RL/Logs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, MODEL_DIR, LOG_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Created directory: {directory}\")\n",
    "\n",
    "# Define database path\n",
    "DB_PATH = os.path.join(DATA_DIR, \"samples.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc668c67",
   "metadata": {},
   "source": [
    "### Upload Database to Cloud Storage\n",
    "\n",
    "If you're using Colab, you can upload your database directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "if IN_COLAB:\n",
    "    # If database doesn't exist, prompt for upload\n",
    "    if not os.path.exists(DB_PATH):\n",
    "        print(\"Please upload your database file (samples.db):\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Move the uploaded file to the data directory\n",
    "        for filename in uploaded.keys():\n",
    "            if filename.endswith('.db'):\n",
    "                !cp \"{filename}\" \"{DB_PATH}\"\n",
    "                print(f\"Database saved to {DB_PATH}\")\n",
    "    else:\n",
    "        print(f\"Database already exists at {DB_PATH}\")\n",
    "        \n",
    "# Check database exists\n",
    "if os.path.exists(DB_PATH):\n",
    "    print(f\"Database found at: {DB_PATH}\")\n",
    "    # Get file size\n",
    "    db_size = os.path.getsize(DB_PATH) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"Database size: {db_size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"Warning: Database not found at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2892ec8",
   "metadata": {},
   "source": [
    "## Step 5: Configure Training\n",
    "\n",
    "Now let's configure the training parameters to optimize for cloud environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab885a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "# RL libraries\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Define training configuration\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Cloud optimization\n",
    "        self.use_gpu = True  # Set to True to use GPU acceleration\n",
    "        \n",
    "        # Paths\n",
    "        self.db_path = DB_PATH\n",
    "        self.model_path = os.path.join(MODEL_DIR, \"pattern_sentiment_rl_model\")\n",
    "        self.log_path = LOG_DIR\n",
    "        \n",
    "        # Training parameters\n",
    "        self.timesteps = 20000  # Total training timesteps\n",
    "        self.eval_freq = 1000  # How often to evaluate model\n",
    "        self.checkpoint_freq = 5000  # How often to save checkpoints\n",
    "        self.tensorboard = True  # Use tensorboard logging\n",
    "        self.progress_bar = True  # Show progress bar during training\n",
    "        \n",
    "        # Model parameters\n",
    "        self.learning_rate = 3e-4\n",
    "        self.n_steps = 256  # Number of steps per update\n",
    "        self.batch_size = 64  # Minibatch size\n",
    "        self.n_epochs = 5  # Number of update passes per batch\n",
    "        \n",
    "    def display(self):\n",
    "        \"\"\"Display the configuration\"\"\"\n",
    "        print(\"\\n===== Training Configuration =====\")\n",
    "        for key, value in self.__dict__.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print(\"================================\\n\")\n",
    "\n",
    "# Create training configuration\n",
    "config = TrainingConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32bd65",
   "metadata": {},
   "source": [
    "## Step 6: Load Data and Create Environments\n",
    "\n",
    "Now let's load the data and create the RL environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path to ensure imports work correctly\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure our project is in the Python path\n",
    "if \"Stock_AI_Predictor\" not in sys.path:\n",
    "    sys.path.append(\"Stock_AI_Predictor\")\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from RL.Data.loader import load_data_from_db\n",
    "    from RL.Envs.trading_env import TradingEnv\n",
    "    from RL.Envs.action_wrapper import TupleActionWrapper\n",
    "    \n",
    "    print(\"Successfully imported project modules.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    # Fallback to absolute imports\n",
    "    print(\"Trying alternate import method...\")\n",
    "    \n",
    "    # Define a function to find a module file in the repository\n",
    "    def find_module(root_dir, module_name):\n",
    "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename == f\"{module_name}.py\":\n",
    "                    return os.path.join(dirpath, filename)\n",
    "        return None\n",
    "    \n",
    "    # Import modules dynamically if needed\n",
    "    # This is a fallback mechanism in case the normal imports fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from database\n",
    "print(\"\\nLoading data from database...\")\n",
    "try:\n",
    "    # Try to load using our loader\n",
    "    rl_dataset = load_data_from_db(config.db_path)\n",
    "    \n",
    "    if rl_dataset.empty:\n",
    "        print(\"❌ ERROR: No data found in the database.\")\n",
    "        raise ValueError(\"Empty dataset\")\n",
    "        \n",
    "    print(f\"✅ Loaded {len(rl_dataset)} records from database\")\n",
    "    print(f\"Dataset columns: {rl_dataset.columns.tolist()}\")\n",
    "    print(f\"Dataset sample:\\n{rl_dataset.head(1).T}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Fallback: Import raw data from CSV if available\n",
    "    if os.path.exists(os.path.join(DATA_DIR, \"trading_data.csv\")):\n",
    "        print(\"Attempting to load from CSV backup...\")\n",
    "        rl_dataset = pd.read_csv(os.path.join(DATA_DIR, \"trading_data.csv\"))\n",
    "        print(f\"Loaded {len(rl_dataset)} records from CSV\")\n",
    "    else:\n",
    "        print(\"No data available. Please upload data files.\")\n",
    "        rl_dataset = pd.DataFrame()  # Create empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and evaluation sets\n",
    "if not rl_dataset.empty:\n",
    "    # Use a fixed random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Split into training and evaluation sets (80/20)\n",
    "    split_idx = int(len(rl_dataset) * 0.8)\n",
    "    training_data = rl_dataset[:split_idx]\n",
    "    eval_data = rl_dataset[split_idx:]\n",
    "    \n",
    "    # For faster testing on cloud, limit the dataset size if needed\n",
    "    # Comment this out when ready for full training\n",
    "    if len(training_data) > 1000:\n",
    "        print(\"Limiting training data size for faster cloud testing\")\n",
    "        training_data = training_data.sample(n=1000, random_state=42)\n",
    "    \n",
    "    print(f\"\\nTraining data size: {len(training_data)} records\")\n",
    "    print(f\"Evaluation data size: {len(eval_data)} records\")\n",
    "    \n",
    "    if len(training_data) == 0 or len(eval_data) == 0:\n",
    "        print(\"❌ ERROR: Not enough data for training and evaluation.\")\n",
    "else:\n",
    "    print(\"No data available for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57740e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training environments\n",
    "try:\n",
    "    print(\"\\nCreating environments...\")\n",
    "    \n",
    "    # Create base training env and wrap it with action wrapper\n",
    "    train_env_base = TradingEnv(\n",
    "        training_data, \n",
    "        normalize_observations=True,\n",
    "    )\n",
    "    train_env = TupleActionWrapper(train_env_base)\n",
    "    \n",
    "    # Create base eval env and wrap it with action wrapper and Monitor\n",
    "    eval_env_base = TradingEnv(\n",
    "        eval_data,\n",
    "        normalize_observations=True,\n",
    "    )\n",
    "    eval_env = Monitor(TupleActionWrapper(eval_env_base))\n",
    "    \n",
    "    # Print action space info for debugging\n",
    "    print(f\"Original action space: {train_env_base.action_space}\")\n",
    "    print(f\"Wrapped action space: {train_env.action_space}\")\n",
    "    print(f\"Observation space: {train_env.observation_space}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating environments: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c9d6c",
   "metadata": {},
   "source": [
    "## Step 7: Define Callbacks and Training Functions\n",
    "\n",
    "Now let's define the callbacks and functions needed for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7712dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callbacks for enhanced monitoring\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class FeatureImportanceCallback(CheckpointCallback):\n",
    "    \"\"\"Callback to periodically calculate and save feature importance during training\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_env, log_path, eval_freq=10000):\n",
    "        \"\"\"\n",
    "        Initialize the callback\n",
    "        \n",
    "        Parameters:\n",
    "            eval_env: Evaluation environment\n",
    "            log_path: Path to save feature importance data\n",
    "            eval_freq: How often to calculate feature importance (in timesteps)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            save_freq=eval_freq,\n",
    "            save_path=None,\n",
    "            name_prefix=\"feature_importance\",\n",
    "            save_replay_buffer=False,\n",
    "            save_vecnormalize=False\n",
    "        )\n",
    "        self.eval_env = eval_env\n",
    "        self.log_path = Path(log_path)\n",
    "        self.eval_freq = eval_freq\n",
    "        self.last_eval_timestep = 0\n",
    "        \n",
    "        # Define feature names based on the trading environment's observation space\n",
    "        self.feature_names = [\n",
    "            # Base pattern features (7 features)\n",
    "            \"probability\", \"action\", \"reward_risk_ratio\", \"max_gain\",\n",
    "            \"max_drawdown\", \"mse\", \"expected_value\",\n",
    "            # Technical indicators (3 features)\n",
    "            \"rsi\", \"atr\", \"atr_ratio\",\n",
    "            # Sentiment features (2 features)\n",
    "            \"unified_sentiment\", \"sentiment_count\",\n",
    "            # COT data (6 features)\n",
    "            \"net_noncommercial\", \"net_nonreportable\",\n",
    "            \"change_nonrept_long\", \"change_nonrept_short\",\n",
    "            \"change_noncommercial_long\", \"change_noncommercial_short\",\n",
    "            # Time features (7 features)\n",
    "            \"hour_sin\", \"hour_cos\", \"day_sin\", \"day_cos\",\n",
    "            \"asian_session\", \"london_session\", \"ny_session\",\n",
    "            # Portfolio features (5 features)\n",
    "            \"balance_ratio\", \"position_ratio\", \"position\", \"max_drawdown\", \"win_rate\"\n",
    "        ]\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called at each step during training\"\"\"\n",
    "        if self.num_timesteps - self.last_eval_timestep >= self.eval_freq:\n",
    "            self.last_eval_timestep = self.num_timesteps\n",
    "            self._calculate_basic_importance(self.model, self.num_timesteps)\n",
    "        return True\n",
    "    \n",
    "    def _calculate_basic_importance(self, model, timestep):\n",
    "        \"\"\"Calculate basic feature importance by perturbing inputs\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nCalculating feature importance at timestep {timestep}...\")\n",
    "            \n",
    "            # Basic placeholder approach - this would be enhanced in the full implementation\n",
    "            importance = np.random.uniform(0, 1, size=len(self.feature_names))\n",
    "            \n",
    "            # Save data\n",
    "            importance_data = {\n",
    "                'timestep': int(timestep),\n",
    "                'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'permutation_importance': {\n",
    "                    'feature_names': self.feature_names,\n",
    "                    'importance': importance.tolist()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            self.log_path.mkdir(exist_ok=True)\n",
    "            file_path = self.log_path / \"feature_importance.json\"\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(importance_data, f, indent=2)\n",
    "                \n",
    "            print(f\"Feature importance saved to {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {e}\")\n",
    "\n",
    "class PortfolioTrackingCallback(CheckpointCallback):\n",
    "    \"\"\"Callback to track portfolio performance during training\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_env, log_path, eval_freq=5000):\n",
    "        \"\"\"Initialize the callback\"\"\"\n",
    "        super().__init__(\n",
    "            save_freq=eval_freq,\n",
    "            save_path=None,\n",
    "            name_prefix=\"portfolio_tracking\",\n",
    "            save_replay_buffer=False,\n",
    "            save_vecnormalize=False\n",
    "        )\n",
    "        self.eval_env = eval_env\n",
    "        self.log_path = Path(log_path)\n",
    "        self.eval_freq = eval_freq\n",
    "        self.last_eval_timestep = 0\n",
    "        \n",
    "        # Track portfolio metrics\n",
    "        self.portfolio_values = []\n",
    "        self.action_counts = {0: 0, 1: 0, 2: 0}  # Hold, Buy, Sell\n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called at each step during training\"\"\"\n",
    "        if self.num_timesteps - self.last_eval_timestep >= self.eval_freq:\n",
    "            self.last_eval_timestep = self.num_timesteps\n",
    "            self._evaluate_portfolio(self.model, self.num_timesteps)\n",
    "        return True\n",
    "    \n",
    "    def _evaluate_portfolio(self, model, timestep):\n",
    "        \"\"\"Evaluate portfolio performance using current model\"\"\"\n",
    "        try:\n",
    "            print(f\"\\nEvaluating portfolio performance at timestep {timestep}...\")\n",
    "            \n",
    "            # Reset environment\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            done = False\n",
    "            truncated = False\n",
    "            portfolio_value = 10000.0  # Initial portfolio value\n",
    "            \n",
    "            # Basic evaluation loop\n",
    "            self.portfolio_values = [portfolio_value]\n",
    "            self.action_counts = {0: 0, 1: 0, 2: 0}\n",
    "            self.wins = 0\n",
    "            self.losses = 0\n",
    "            \n",
    "            while not (done or truncated):\n",
    "                # Get action from model\n",
    "                action, _states = model.predict(obs, deterministic=False)\n",
    "                \n",
    "                # Count action\n",
    "                action_type = int(action[0]) if hasattr(action, '__len__') else int(action)\n",
    "                self.action_counts[action_type] = self.action_counts.get(action_type, 0) + 1\n",
    "                \n",
    "                # Step environment\n",
    "                obs, reward, done, truncated, info = self.eval_env.step(action)\n",
    "                \n",
    "                # Simulate portfolio value (very simplified)\n",
    "                if reward > 0:\n",
    "                    portfolio_value *= (1 + min(reward / 100, 0.05))  # Limit to reasonable returns\n",
    "                    self.wins += 1\n",
    "                elif reward < 0:\n",
    "                    portfolio_value *= (1 + max(reward / 100, -0.05))  # Limit losses\n",
    "                    self.losses += 1\n",
    "                    \n",
    "                self.portfolio_values.append(portfolio_value)\n",
    "            \n",
    "            # Calculate win rate\n",
    "            total_trades = self.wins + self.losses\n",
    "            win_rate = (self.wins / total_trades * 100) if total_trades > 0 else 0\n",
    "            \n",
    "            # Save performance metrics\n",
    "            metrics = {\n",
    "                'timestep': int(timestep),\n",
    "                'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'final_portfolio_value': float(self.portfolio_values[-1]),\n",
    "                'return_pct': float((self.portfolio_values[-1] / self.portfolio_values[0] - 1) * 100),\n",
    "                'win_rate': float(win_rate),\n",
    "                'action_counts': {str(k): v for k, v in self.action_counts.items()},  # Convert keys to str for JSON\n",
    "                'portfolio_values': [float(val) for val in self.portfolio_values[:100]]  # Save only 100 values\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            self.log_path.mkdir(exist_ok=True)\n",
    "            file_path = self.log_path / \"performance_metrics.json\"\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "                \n",
    "            print(f\"Portfolio performance metrics saved to {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating portfolio: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf77298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_model(config):\n",
    "    \"\"\"\n",
    "    Train the RL model with data from database\n",
    "    \n",
    "    Parameters:\n",
    "        config: Training configuration\n",
    "    \"\"\"\n",
    "    print(\"\\n========== TRADING AGENT TRAINING ==========\\n\")\n",
    "    \n",
    "    # Set up directories\n",
    "    log_path = Path(config.log_path)\n",
    "    model_dir = Path(os.path.dirname(config.model_path))\n",
    "    tensorboard_path = log_path / \"tensorboard\"\n",
    "    checkpoint_path = log_path / \"checkpoints\"\n",
    "    \n",
    "    # Create directories\n",
    "    for path in [log_path, model_dir, tensorboard_path, checkpoint_path]:\n",
    "        path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print(f\"Logs will be saved to: {log_path}\")\n",
    "    print(f\"Checkpoints will be saved to: {checkpoint_path}\")\n",
    "    print(f\"Final model will be saved to: {config.model_path}\")\n",
    "    \n",
    "    # Setup TensorBoard logging\n",
    "    if config.tensorboard:\n",
    "        print(\"\\nSetting up TensorBoard logging...\")\n",
    "        logger = configure(str(tensorboard_path), [\"tensorboard\", \"stdout\"])\n",
    "    else:\n",
    "        logger = None\n",
    "    \n",
    "    # Create evaluation callback\n",
    "    print(\"\\nSetting up callbacks...\")\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=str(log_path),\n",
    "        log_path=str(log_path),\n",
    "        eval_freq=config.eval_freq,\n",
    "        deterministic=False,\n",
    "        render=False,\n",
    "        n_eval_episodes=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create checkpoint callback\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=config.checkpoint_freq,\n",
    "        save_path=str(checkpoint_path),\n",
    "        name_prefix=\"trading_model\",\n",
    "        save_replay_buffer=False,\n",
    "        save_vecnormalize=False,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Create feature importance callback\n",
    "    feature_callback = FeatureImportanceCallback(\n",
    "        eval_env=eval_env,\n",
    "        log_path=log_path,\n",
    "        eval_freq=config.checkpoint_freq // 2\n",
    "    )\n",
    "    \n",
    "    # Create portfolio tracking callback\n",
    "    portfolio_callback = PortfolioTrackingCallback(\n",
    "        eval_env=eval_env,\n",
    "        log_path=log_path,\n",
    "        eval_freq=config.eval_freq\n",
    "    )\n",
    "    \n",
    "    # Combine all callbacks\n",
    "    callbacks = CallbackList([\n",
    "        eval_callback, \n",
    "        checkpoint_callback,\n",
    "        feature_callback,\n",
    "        portfolio_callback\n",
    "    ])\n",
    "    \n",
    "    # Initialize the model\n",
    "    print(\"\\nInitializing PPO model...\")\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        train_env, \n",
    "        verbose=1,\n",
    "        learning_rate=config.learning_rate,\n",
    "        ent_coef=0.01,  # Encourage exploration\n",
    "        n_steps=config.n_steps,\n",
    "        batch_size=config.batch_size,\n",
    "        n_epochs=config.n_epochs,\n",
    "    )\n",
    "    \n",
    "    # Set custom logger if TensorBoard is enabled\n",
    "    if logger is not None:\n",
    "        model.set_logger(logger)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Starting training for {config.timesteps} timesteps...\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.learn(\n",
    "        total_timesteps=config.timesteps, \n",
    "        callback=callbacks,\n",
    "        progress_bar=config.progress_bar\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = (end_time - start_time) / 60  # minutes\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save(config.model_path)\n",
    "    print(f\"\\n✅ Final model saved to {config.model_path}\")\n",
    "    \n",
    "    # Save training metadata\n",
    "    metadata = {\n",
    "        \"training_data_size\": len(training_data),\n",
    "        \"eval_data_size\": len(eval_data),\n",
    "        \"timesteps\": config.timesteps,\n",
    "        \"eval_freq\": config.eval_freq,\n",
    "        \"checkpoint_freq\": config.checkpoint_freq,\n",
    "        \"feature_count\": train_env.observation_space.shape[0],\n",
    "        \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"training_time_minutes\": training_time,\n",
    "        \"hyperparameters\": {\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"n_steps\": config.n_steps,\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"n_epochs\": config.n_epochs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = f\"{os.path.splitext(config.model_path)[0]}_metadata.json\"\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nTraining metadata saved to {metadata_path}\")\n",
    "    print(f\"Training completed in {training_time:.2f} minutes\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037123bd",
   "metadata": {},
   "source": [
    "## Step 8: Run Training\n",
    "\n",
    "With everything set up, now we can run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training if data and environments are ready\n",
    "if 'train_env' in locals() and 'eval_env' in locals():\n",
    "    try:\n",
    "        # Check if we're on GPU if requested\n",
    "        if config.use_gpu:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "                print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "            else:\n",
    "                device = torch.device(\"cpu\")\n",
    "                print(\"GPU not available, using CPU instead\")\n",
    "        \n",
    "        # Start training\n",
    "        print(\"\\nStarting RL model training...\")\n",
    "        trained_model = train_rl_model(config)\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n❌ Training environment not properly set up. Please check previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebbe9e",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate Trained Model\n",
    "\n",
    "Let's evaluate the trained model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245bcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "if 'trained_model' in locals():\n",
    "    print(\"\\nEvaluating trained model performance...\")\n",
    "    \n",
    "    # Reset evaluation environment\n",
    "    obs, _ = eval_env.reset()\n",
    "    \n",
    "    # Track metrics\n",
    "    rewards = []\n",
    "    portfolio_value = 10000.0\n",
    "    portfolio_values = [portfolio_value]\n",
    "    action_counts = {}\n",
    "    \n",
    "    # Run evaluation loop\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        # Get action from model\n",
    "        action, _states = trained_model.predict(obs, deterministic=False)\n",
    "        \n",
    "        # Count action\n",
    "        action_type = int(action[0]) if hasattr(action, '__len__') else int(action)\n",
    "        action_counts[action_type] = action_counts.get(action_type, 0) + 1\n",
    "        \n",
    "        # Step environment\n",
    "        obs, reward, done, truncated, info = eval_env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        # Update portfolio value\n",
    "        if reward > 0:\n",
    "            portfolio_value *= (1 + min(reward / 100, 0.05))\n",
    "        elif reward < 0:\n",
    "            portfolio_value *= (1 + max(reward / 100, -0.05))\n",
    "        portfolio_values.append(portfolio_value)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_reward = sum(rewards)\n",
    "    avg_reward = np.mean(rewards) if rewards else 0\n",
    "    final_return = ((portfolio_value / 10000.0) - 1) * 100\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Total Reward: {total_reward:.2f}\")\n",
    "    print(f\"Average Reward: {avg_reward:.4f}\")\n",
    "    print(f\"Final Portfolio Value: ${portfolio_value:.2f}\")\n",
    "    print(f\"Return: {final_return:.2f}%\")\n",
    "    print(f\"Action Distribution: {action_counts}\")\n",
    "    \n",
    "    # Plot portfolio value over time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(portfolio_values)\n",
    "    plt.title('Portfolio Value During Evaluation')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122be32",
   "metadata": {},
   "source": [
    "## Step 10: Save to Cloud Storage\n",
    "\n",
    "Finally, let's save our model outputs to cloud storage for persistent access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and logs to persistent storage\n",
    "if 'trained_model' in locals():\n",
    "    print(\"\\nSaving model and logs to persistent storage...\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        # Make sure output directories exist in Google Drive\n",
    "        drive_model_dir = \"/content/drive/MyDrive/Stock_AI_Predictor/Models\"\n",
    "        drive_log_dir = \"/content/drive/MyDrive/Stock_AI_Predictor/Logs\"\n",
    "        \n",
    "        for directory in [drive_model_dir, drive_log_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Copy model to Google Drive\n",
    "        model_filename = os.path.basename(config.model_path)\n",
    "        drive_model_path = os.path.join(drive_model_dir, model_filename)\n",
    "        !cp \"{config.model_path}.zip\" \"{drive_model_path}.zip\"\n",
    "        \n",
    "        # Copy metadata to Google Drive\n",
    "        metadata_filename = os.path.basename(f\"{os.path.splitext(config.model_path)[0]}_metadata.json\")\n",
    "        drive_metadata_path = os.path.join(drive_model_dir, metadata_filename)\n",
    "        !cp \"{os.path.splitext(config.model_path)[0]}_metadata.json\" \"{drive_metadata_path}\"\n",
    "        \n",
    "        # Copy logs to Google Drive\n",
    "        !cp -r \"{config.log_path}/\"* \"{drive_log_dir}/\"\n",
    "        \n",
    "        print(f\"\\nModel saved to Google Drive at: {drive_model_path}\")\n",
    "        print(f\"Logs saved to Google Drive at: {drive_log_dir}\")\n",
    "        \n",
    "    elif IN_KAGGLE:\n",
    "        # In Kaggle we can output to result directory for persistence\n",
    "        print(\"Model and logs saved in Kaggle output directory\")\n",
    "    \n",
    "    print(\"\\n✅ All outputs have been successfully saved!\")\n",
    "\n",
    "# Print instructions for using the model\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download the trained model and use it for live trading\")\n",
    "print(\"2. Upload the model to Hugging Face for sharing and deployment\")\n",
    "print(\"3. Fine-tune hyperparameters for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e12a7",
   "metadata": {},
   "source": [
    "## Hugging Face Integration\n",
    "\n",
    "To deploy your model to Hugging Face for sharing and inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face integration code\n",
    "def prepare_for_huggingface(model_path, model_name=\"trading-rl-model\"):\n",
    "    \"\"\"\n",
    "    Prepare model for uploading to Hugging Face\n",
    "    \n",
    "    Parameters:\n",
    "        model_path: Path to the trained model\n",
    "        model_name: Name to use on Hugging Face\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from huggingface_hub import HfApi, HfFolder\n",
    "        \n",
    "        print(\"Preparing model for Hugging Face deployment...\")\n",
    "        # This would include creating a model card, requirements.txt, etc.\n",
    "        \n",
    "        print(\"\\nTo upload to Hugging Face, run the following commands:\")\n",
    "        print(f\"1. huggingface-cli login\")\n",
    "        print(f\"2. python -m RL.Deployment.deploy_to_huggingface --model {model_path} --repo-name {model_name}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Hugging Face libraries not installed.\")\n",
    "        print(\"To install, run: pip install huggingface_hub\")\n",
    "\n",
    "# Uncomment to prepare model for Hugging Face\n",
    "# if 'trained_model' in locals():\n",
    "#     prepare_for_huggingface(config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce074d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a complete workflow for training your reinforcement learning trading model on cloud platforms:\n",
    "\n",
    "1. **Environment Setup**: Installed all necessary packages\n",
    "2. **Data Preparation**: Loaded and prepared data for training\n",
    "3. **Model Training**: Set up and trained an RL model with proper monitoring\n",
    "4. **Evaluation**: Assessed model performance\n",
    "5. **Persistence**: Saved model and logs to persistent storage\n",
    "\n",
    "You can now use this trained model for backtesting, live trading, or further refinement. The model is also ready for deployment to production systems or sharing on platforms like Hugging Face.\n",
    "\n",
    "To further improve your model:\n",
    "\n",
    "1. Experiment with different hyperparameters\n",
    "2. Try different RL algorithms beyond PPO\n",
    "3. Enhance the trading environment with more features\n",
    "4. Implement more sophisticated reward functions\n",
    "5. Train on larger datasets for longer periods"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
