{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f35f3673c9c053",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cd1465dfba10c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to sqlite cloud database\n",
      "Found [151] tweets in database for period [2025-01-18] to [2025-01-18] for ticker_id: 3 and ticker_name: AAPL\n",
      "Sufficient tweets found in database. Proceeding with analysis...\n",
      "Ready for analysis with date range: 2025-01-18 to 2025-01-18, ticker ID: 3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Get the current working directory (where the notebook/script is running)\n",
    "current_dir = Path(os.getcwd())\n",
    "# Navigate to the 'main' folder (adjust if needed)\n",
    "main_dir = str(current_dir.parent)  # If notebook is inside 'main'\n",
    "# OR if notebook is outside 'main':\n",
    "# main_dir = str(current_dir / \"main\")  # Assumes 'main' is a subfolder\n",
    "sys.path.append(main_dir)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from Data.db_cloud import Database\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize database connection\n",
    "db = Database()\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "main_dir = str(current_dir.parent)\n",
    "sys.path.append(main_dir)\n",
    "# Load environment variables (for API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Token from environment variable\n",
    "TWITTER_API_KEY = os.getenv(\"TWITTER_API_KEY\")\n",
    "\n",
    "# Define base URL for Twitter API\n",
    "TWITTER_API_BASE_URL = \"https://api.twitterapi.io/twitter/tweet/advanced_search\"\n",
    "\n",
    "# Define ticker mapping\n",
    "TICKER_MAPPING = {\n",
    "    1: \"XAUUSD\",  # Gold\n",
    "    2: \"BTCUSD\",  # Bitcoin\n",
    "    3: \"AAPL\",    # Apple\n",
    "    4: \"AMZN\",    # Amazon\n",
    "    5: \"NVDA\"     # Nvidia\n",
    "}\n",
    "\n",
    "# Define search queries based on ticker ID\n",
    "def get_search_queries_by_ticker(ticker_id):\n",
    "    \"\"\"\n",
    "    Get appropriate search queries based on ticker ID\n",
    "    \n",
    "    Args:\n",
    "        ticker_id (int): ID of the ticker\n",
    "        \n",
    "    Returns:\n",
    "        list: List of search queries\n",
    "    \"\"\"\n",
    "    if ticker_id not in TICKER_MAPPING:\n",
    "        print(f\"Error: Ticker ID {ticker_id} not found in mapping\")\n",
    "        return []\n",
    "    \n",
    "    ticker = TICKER_MAPPING[ticker_id]\n",
    "    # Gold\n",
    "    if ticker == \"XAUUSD\":  \n",
    "        return [\n",
    "            \"XAUUSD OR Gold price min_retweets:20\",\n",
    "            \"Gold trading OR Gold market min_retweets:20\",\n",
    "            \"Gold investment min_retweets:20\",\n",
    "        ]\n",
    "    # Bitcoin\n",
    "    elif ticker == \"BTCUSD\":  \n",
    "        return [\n",
    "            \"BTCUSD OR Bitcoin price min_retweets:20\",\n",
    "            \"Bitcoin trading OR Bitcoin market min_retweets:20\",\n",
    "        ]\n",
    "    # Apple\n",
    "    elif ticker == \"AAPL\":  \n",
    "        return [\n",
    "            \"AAPL OR $AAPL -from:Apple min_retweets:20\",\n",
    "            \"Tim Cook -from:Apple min_retweets:20\",\n",
    "        ]\n",
    "    elif ticker == \"AMZN\":  # Amazon\n",
    "        return [\n",
    "            \"AMZN OR Amazon -from:Amazon min_retweets:20\",\n",
    "            \"Jeff Bezos OR Andy Jassy -from:Amazon min_retweets:20\",\n",
    "        ]\n",
    "    elif ticker == \"NVDA\":  # Nvidia\n",
    "        return [\n",
    "            \"NVDA OR Nvidia -from:Nvidia min_retweets:20\",\n",
    "            \"Jensen Huang -from:Nvidia min_retweets:20\",\n",
    "        ]\n",
    "\n",
    "\n",
    "    # elif ticker == \"MSFT\":  # Microsoft\n",
    "    #     return [\n",
    "    #         \"MSFT OR Microsoft -from:Microsoft\",\n",
    "    #         \"Satya Nadella -from:Microsoft\",\n",
    "    #         \"Windows min_retweets:10\",\n",
    "    #         \"Xbox OR (Microsoft Gaming) min_likes:25\",\n",
    "    #         \"Surface OR (Surface Book) OR (Surface Pro)\",\n",
    "    #         \"Microsoft 365 OR Office365\",\n",
    "    #         \"Azure OR (Microsoft Cloud)\",\n",
    "    #         \"Microsoft earnings OR (MSFT earnings)\",\n",
    "    #         \"(Microsoft stock) OR (MSFT stock) OR (Microsoft shares)\",\n",
    "    #         \"Microsoft AI OR (Microsoft Copilot)\",\n",
    "    #         \"Microsoft Teams\",\n",
    "    #         \"GitHub\"\n",
    "    #     ]\n",
    "    # elif ticker == \"GOOGL\":  # Google\n",
    "    #     return [\n",
    "    #         \"GOOGL OR Google -from:Google\",\n",
    "    #         \"Sundar Pichai -from:Google\",\n",
    "    #         \"Android min_retweets:10\",\n",
    "    #         \"Google I/O OR (Google event) min_likes:25\",\n",
    "    #         \"Pixel OR (Google Pixel) OR (Pixel Pro)\",\n",
    "    #         \"Chrome OR ChromeOS\",\n",
    "    #         \"Google Cloud\",\n",
    "    #         \"Google earnings OR (GOOGL earnings)\",\n",
    "    #         \"(Google stock) OR (GOOGL stock) OR (Google shares)\",\n",
    "    #         \"Google AI OR (Google Gemini)\",\n",
    "    #         \"YouTube\"\n",
    "    #     ]\n",
    "    # Add more ticker query sets as needed\n",
    "    else:\n",
    "        # Generic queries for any ticker\n",
    "        return [\n",
    "            f\"{ticker}\",\n",
    "            f\"${ticker}\",\n",
    "            f\"{ticker} price\",\n",
    "            f\"{ticker} trading\",\n",
    "            f\"{ticker} stock\",\n",
    "            f\"{ticker} market\",\n",
    "            f\"{ticker} analysis\",\n",
    "            f\"{ticker} forecast\",\n",
    "            f\"{ticker} earnings\",\n",
    "            f\"{ticker} news\"\n",
    "        ]\n",
    "\n",
    "def check_tweets_in_db_for_date(start_date, end_date, ticker_id=0):\n",
    "    \"\"\"\n",
    "    Check if tweets exist in DB for the specified date range and ticker\n",
    "    \n",
    "    Args:\n",
    "        start_date (str): Start date in YYYY-MM-DD format\n",
    "        end_date (str): End date in YYYY-MM-DD format\n",
    "        ticker_id (int, optional): ID of the ticker\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (bool, int) - (exists, count)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to datetime objects\n",
    "        start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        \n",
    "        # Add one day to end_date to include the full day\n",
    "        end_dt = end_dt + timedelta(days=1)\n",
    "        \n",
    "        # Format for database query\n",
    "        start_str = start_dt.strftime('%Y-%m-%d')\n",
    "        end_str = end_dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Query database\n",
    "        tweets_df = db.get_tweets_count_by_date_and_ticker(\n",
    "            start_date=start_str,\n",
    "            end_date=end_str,\n",
    "            ticker_id=ticker_id\n",
    "        )\n",
    "        \n",
    "        # Calculate total count\n",
    "        tweet_count = tweets_df['tweet_count'].sum() if not tweets_df.empty else 0\n",
    "        \n",
    "        # Check if tweets exist\n",
    "        exists = tweet_count > 0\n",
    "        \n",
    "        ticker_name = TICKER_MAPPING.get(ticker_id, f\"ticker {ticker_id}\") if ticker_id in TICKER_MAPPING else \"\"\n",
    "        print(f\"Found [{tweet_count}] tweets in database for period [{start_date}] to [{end_date}]\" +\n",
    "              (f\" for ticker_id: {ticker_id} and ticker_name: {ticker_name}\" if ticker_name else \"\"))\n",
    "        \n",
    "        return exists, tweet_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking tweets in database: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "def fetch_tweets(query, date_from=None, date_to=None, query_type=\"Latest\", max_tweets=1000):\n",
    "    \"\"\"\n",
    "    Fetch tweets using the TwitterAPI.io Advanced Search endpoint\n",
    "    \n",
    "    Args:\n",
    "        query (str): Twitter advanced search query\n",
    "        date_from (str, optional): Start date in YYYY-MM-DD format\n",
    "        date_to (str, optional): End date in YYYY-MM-DD format\n",
    "        query_type (str): \"Latest\" or \"Top\" tweets\n",
    "        max_tweets (int): Maximum number of tweets to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tweet objects\n",
    "    \"\"\"\n",
    "    all_tweets = []\n",
    "    cursor = \"\"\n",
    "    page_count = 0\n",
    "    max_pages = (max_tweets // 100) + 1  # API typically returns ~100 tweets per page\n",
    "    \n",
    "    # Modify query to include date range if provided\n",
    "    if date_from and date_to:\n",
    "        date_from_str = datetime.strptime(date_from, '%Y-%m-%d').strftime('%Y-%m-%d_%H:%M:%S_UTC')\n",
    "        date_to_str = datetime.strptime(date_to, '%Y-%m-%d').strftime('%Y-%m-%d_%H:%M:%S_UTC')\n",
    "        # Add one day to include the full end date\n",
    "        date_to_dt = datetime.strptime(date_to, '%Y-%m-%d') + timedelta(days=1)\n",
    "        date_to_str = date_to_dt.strftime('%Y-%m-%d_%H:%M:%S_UTC')\n",
    "        query = f\"{query} since:{date_from_str} until:{date_to_str}\"\n",
    "        \n",
    "    print(f\"Fetching tweets for query: {query}\")\n",
    "    \n",
    "    while len(all_tweets) < max_tweets and page_count < max_pages:\n",
    "        # Prepare request parameters\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"queryType\": query_type\n",
    "        }\n",
    "        \n",
    "        # Add cursor for pagination if not first page\n",
    "        if cursor:\n",
    "            params[\"cursor\"] = cursor\n",
    "        \n",
    "        # Prepare headers with API key\n",
    "        headers = {\n",
    "            \"X-API-Key\": f\"{TWITTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Make API request\n",
    "            response = requests.get(\n",
    "                TWITTER_API_BASE_URL,\n",
    "                params=params,\n",
    "                headers=headers\n",
    "            )\n",
    "            \n",
    "            # Check for successful response\n",
    "            response.raise_for_status() # If the response is not 200, this will raise an HTTPError\n",
    "            \n",
    "            # Parse response\n",
    "            data = response.json() \n",
    "            \n",
    "            # Add tweets to our collection\n",
    "            new_tweets = data.get(\"tweets\", [])\n",
    "            all_tweets.extend(new_tweets)\n",
    "            \n",
    "            # Update cursor for next page\n",
    "            cursor = data.get(\"next_cursor\", \"\")\n",
    "            has_next_page = data.get(\"has_next_page\", False)\n",
    "            \n",
    "            page_count += 1\n",
    "            \n",
    "            print(f\"Page {page_count}: Retrieved {len(new_tweets)} tweets. Total: {len(all_tweets)}\")\n",
    "            \n",
    "            # If no more pages or we have enough tweets, break\n",
    "            if not has_next_page or not cursor:\n",
    "                print(f\"No more pages for {query} or cursor is empty. Stopping fetch.\")\n",
    "                break\n",
    "                \n",
    "            # Be nice to the API\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching tweets: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_tweets\n",
    "\n",
    "def process_tweet(tweet, search_term, ticker_id=0):\n",
    "    \"\"\"\n",
    "    Process a single tweet and extract relevant fields without sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        tweet (dict): Tweet object from the API\n",
    "        search_term (str): The search term that found this tweet\n",
    "        ticker_id (int, optional): ID of the ticker\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed tweet data\n",
    "    \"\"\"\n",
    "    # Current timestamp\n",
    "    current_time = datetime.now().isoformat()\n",
    "    \n",
    "    # Convert tweet creation time to ISO format\n",
    "    created_at = tweet.get('createdAt', '')\n",
    "    if created_at:\n",
    "        created_at = datetime.strptime(created_at, \"%a %b %d %H:%M:%S %z %Y\").isoformat()\n",
    "        \n",
    "    # Extract empty placeholders for sentiment fields (to be filled later)\n",
    "    sentiment_label = \"\"\n",
    "    sentiment_score = 0.0\n",
    "    sentiment_magnitude = 0.0\n",
    "    weighted_sentiment = 0.0\n",
    "    \n",
    "    # Return processed tweet data\n",
    "    return {\n",
    "        'tweet_id': tweet.get('id', ''),\n",
    "        'tweet_text': tweet.get('text', ''),\n",
    "        'created_at': created_at,\n",
    "        'retweet_count': tweet.get('retweetCount', 0),\n",
    "        'reply_count': tweet.get('replyCount', 0),\n",
    "        'like_count': tweet.get('likeCount', 0),\n",
    "        'quote_count': tweet.get('quoteCount', 0),\n",
    "        'bookmark_count': tweet.get('bookmarkCount', 0),\n",
    "        'lang': tweet.get('lang', ''),\n",
    "        'is_reply': tweet.get('isReply', False),\n",
    "        'is_quote': bool(tweet.get('quoted_tweet')),\n",
    "        'is_retweet': bool(tweet.get('retweeted_tweet')),\n",
    "        'url': tweet.get('url', ''),\n",
    "        'search_term': search_term,\n",
    "        'author_username': tweet.get('author', {}).get('userName', ''),\n",
    "        'author_name': tweet.get('author', {}).get('name', ''),\n",
    "        'author_verified': False,  # TwitterAPI.io doesn't have this field, only isBlueVerified\n",
    "        'author_blue_verified': tweet.get('author', {}).get('isBlueVerified', False),\n",
    "        'author_followers': tweet.get('author', {}).get('followers', 0),\n",
    "        'author_following': tweet.get('author', {}).get('following', 0),\n",
    "        'sentiment_label': sentiment_label,\n",
    "        'sentiment_score': sentiment_score,\n",
    "        'sentiment_magnitude': sentiment_magnitude,\n",
    "        'weighted_sentiment': weighted_sentiment,\n",
    "        'collected_at': current_time,\n",
    "        'ticker_id': ticker_id,\n",
    "    }\n",
    "\n",
    "def store_tweets_in_db(tweets):\n",
    "    \"\"\"\n",
    "    Store processed tweets in the database\n",
    "    \n",
    "    Args:\n",
    "        tweets (list): List of processed tweet dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tweets stored\n",
    "    \"\"\"\n",
    "    stored_count = 0\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            db.store_tweets(\n",
    "                ticker_id=tweet['ticker_id'],\n",
    "                tweet_id=tweet['tweet_id'],\n",
    "                tweet_text=tweet['tweet_text'],\n",
    "                created_at=tweet['created_at'],\n",
    "                retweet_count=tweet['retweet_count'],\n",
    "                reply_count=tweet['reply_count'],\n",
    "                like_count=tweet['like_count'],\n",
    "                quote_count=tweet['quote_count'],\n",
    "                bookmark_count=tweet['bookmark_count'],\n",
    "                lang=tweet['lang'],\n",
    "                is_reply=tweet['is_reply'],\n",
    "                is_quote=tweet['is_quote'],\n",
    "                is_retweet=tweet['is_retweet'],\n",
    "                url=tweet['url'],\n",
    "                search_term=tweet['search_term'],\n",
    "                author_username=tweet['author_username'],\n",
    "                author_name=tweet['author_name'],\n",
    "                author_verified=tweet['author_verified'],\n",
    "                author_blue_verified=tweet['author_blue_verified'],\n",
    "                author_followers=tweet['author_followers'],\n",
    "                author_following=tweet['author_following'],\n",
    "                sentiment_label=tweet['sentiment_label'],\n",
    "                sentiment_score=tweet['sentiment_score'],\n",
    "                sentiment_magnitude=tweet['sentiment_magnitude'],\n",
    "                weighted_sentiment=tweet['weighted_sentiment'],\n",
    "                collected_at=tweet['collected_at'],\n",
    "            )\n",
    "            stored_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error storing tweet {tweet['tweet_id']}: {e}\")\n",
    "    \n",
    "    return stored_count\n",
    "\n",
    "def fetch_and_store_tweets_for_date_range(start_date, end_date, ticker_id=None, tweets_per_query=500, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Fetch tweets for a specific date range and ticker, then store in DB\n",
    "    \n",
    "    Args:\n",
    "        start_date (str): Start date in YYYY-MM-DD format\n",
    "        end_date (str): End date in YYYY-MM-DD format\n",
    "        ticker_id (int, optional): ID of the ticker\n",
    "        tweets_per_query (int): Number of tweets per query\n",
    "        language (str): Language filter\n",
    "        \n",
    "    Returns:\n",
    "        int: Total number of tweets stored\n",
    "    \"\"\"\n",
    "    total_stored = 0\n",
    "    \n",
    "    # Get search queries based on ticker ID\n",
    "    search_queries = get_search_queries_by_ticker(ticker_id) if ticker_id else APPLE_SEARCH_QUERIES\n",
    "    \n",
    "    if not search_queries:\n",
    "        print(f\"No search queries found for ticker ID {ticker_id}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"\\n======= Fetching tweets from {start_date} to {end_date} =======\")\n",
    "    \n",
    "    # Process each search query\n",
    "    for base_query in search_queries:\n",
    "        # Add language filter\n",
    "        query = f\"{base_query} lang:{language}\"\n",
    "        print(f\"Processing query: {query}\")\n",
    "        \n",
    "        # Fetch tweets for this query\n",
    "        tweets = fetch_tweets(\n",
    "            query=query,\n",
    "            date_from=start_date,\n",
    "            date_to=end_date,\n",
    "            query_type=\"Latest\",\n",
    "            max_tweets=tweets_per_query\n",
    "        )\n",
    "        \n",
    "        if tweets:\n",
    "            # Process tweets\n",
    "            processed_tweets = [process_tweet(tweet, base_query, ticker_id) for tweet in tweets]\n",
    "            \n",
    "            # Store in database\n",
    "            stored_count = store_tweets_in_db(processed_tweets)\n",
    "            total_stored += stored_count\n",
    "            \n",
    "            print(f\"Stored {stored_count} tweets for query '{base_query}' for period {start_date} to {end_date}\")\n",
    "        else:\n",
    "            print(f\"No tweets found for query '{base_query}' for period {start_date} to {end_date}\")\n",
    "    \n",
    "    print(f\"### Tweet fetching and storage completed. Total tweets stored: {total_stored} ###\")\n",
    "    return total_stored\n",
    "\n",
    "def fetch_tweets_if_needed(specific_date=None, start_date=None, end_date=None, ticker_id=None, min_tweet_count=100):\n",
    "    \"\"\"\n",
    "    Check if tweets exist for the given date parameters and fetch if needed\n",
    "    \n",
    "    Args:\n",
    "        specific_date (str, optional): A single date to analyze (YYYY-MM-DD)\n",
    "        start_date (str, optional): Start date of range (YYYY-MM-DD)\n",
    "        end_date (str, optional): End date of range (YYYY-MM-DD)\n",
    "        ticker_id (int, optional): ID of the ticker\n",
    "        min_tweet_count (int): Minimum number of tweets needed\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (start_date, end_date, needs_analysis)\n",
    "    \"\"\"\n",
    "    # Process date parameters\n",
    "    if specific_date:\n",
    "        start_date = specific_date\n",
    "        end_date = specific_date\n",
    "    elif start_date and end_date:\n",
    "        # Use provided date range\n",
    "        pass\n",
    "    else:\n",
    "        # Default to yesterday if no date parameters\n",
    "        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        start_date = yesterday\n",
    "        end_date = yesterday\n",
    "        \n",
    "    # Check if we have enough tweets in the database\n",
    "    tweets_exist, tweet_count = check_tweets_in_db_for_date(start_date, end_date, ticker_id)\n",
    "    \n",
    "    if not tweets_exist or tweet_count < min_tweet_count:\n",
    "        print(f\"Insufficient tweets in database for the requested date range.\\n[Tweets_Count={tweet_count}], [Minimum_Tweets_Count={min_tweet_count}]\\n=========Fetching from API...=========\")\n",
    "        fetch_and_store_tweets_for_date_range(start_date, end_date, ticker_id)\n",
    "        return start_date, end_date, True\n",
    "    else:\n",
    "        print(f\"Sufficient tweets found in database. Proceeding with analysis...\")\n",
    "        return start_date, end_date, False\n",
    "\n",
    "# Define Apple search queries (default if no ticker specified)\n",
    "APPLE_SEARCH_QUERIES = [\n",
    "    \"AAPL OR Apple -from:Apple\",\n",
    "    \"Tim Cook -from:Apple\",\n",
    "    \"iPhone min_retweets:10\",\n",
    "    \"WWDC OR (Apple event) min_likes:25\",\n",
    "    \"MacBook OR Macbook OR (Mac Pro) OR iMac\",\n",
    "    \"iPad OR iPadOS\",\n",
    "    \"iOS OR iPadOS OR macOS\",\n",
    "    \"Apple earnings OR (AAPL earnings)\",\n",
    "    \"(Apple stock) OR (AAPL stock) OR (Apple shares)\",\n",
    "    \"Apple AI OR (Apple intelligence)\",\n",
    "    \"Apple Vision Pro\",\n",
    "    \"Apple Watch\",\n",
    "    \"AirPods OR (Apple headphones)\"\n",
    "]\n",
    "\n",
    "# Example usage to analyze a specific date\n",
    "if __name__ == \"__main__\":\n",
    "    # Example specific date\n",
    "    specific_date = '2025-01-18'  # Use specific_date for a single day\n",
    "    # Or date range:\n",
    "    # start_date = '2025-01-01'\n",
    "    # end_date = '2025-01-31'\n",
    "    ticker_id = 3  # AAPL\n",
    "    \n",
    "    # Fetch tweets if needed and get date range for analysis\n",
    "    start_date, end_date, _ = fetch_tweets_if_needed(specific_date=specific_date, ticker_id=ticker_id)\n",
    "    # Alternative: fetch_tweets_if_needed(start_date=start_date, end_date=end_date, ticker_id=ticker_id)\n",
    "    \n",
    "    print(f\"Ready for analysis with date range: {start_date} to {end_date}, ticker ID: {ticker_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5b913f8254192a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to sqlite cloud database\n",
      "Found [151] tweets in database for period [2025-01-18] to [2025-01-18] for ticker_id: 3 and ticker_name: AAPL\n",
      "Sufficient tweets found in database. Proceeding with analysis...\n",
      "Loaded [151] tweets from database for date range 2025-01-18 to 2025-01-18 with ticker AAPL\n",
      "Using CPU.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 530\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# Alternative: use date range\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# start_date = '2025-01-01' \u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# end_date = '2025-01-31'\u001b[39;00m\n\u001b[0;32m    529\u001b[0m ticker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# AAPL\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mget_tweets_sentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mticker_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecific_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecific_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m============ Sentiment Summary ===========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[1;32mIn[8], line 479\u001b[0m, in \u001b[0;36mget_tweets_sentiment_analysis\u001b[1;34m(ticker_id, ticker_symbol, specific_date, start_date, end_date)\u001b[0m\n\u001b[0;32m    476\u001b[0m     end_date \u001b[38;5;241m=\u001b[39m specific_date\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# Analyze or load the data\u001b[39;00m\n\u001b[1;32m--> 479\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_tweets_for_date_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecific_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspecific_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mticker_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mticker_id\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# If dataframe is empty or less than min_tweet_count, return zeros/defaults\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m<\u001b[39m min_tweet_count:\n",
      "Cell \u001b[1;32mIn[8], line 416\u001b[0m, in \u001b[0;36manalyze_tweets_for_date_range\u001b[1;34m(specific_date, start_date, end_date, ticker_id, min_tweet_count)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# Check if tokenizer and model are loaded\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mand\u001b[39;00m model:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;66;03m# Add sentiment scores to DataFrame\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43madd_sentiment_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;66;03m# Calculate engagement-weighted sentiment\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     df \u001b[38;5;241m=\u001b[39m calculate_engagement_weighted_sentiment(\n\u001b[0;32m    420\u001b[0m         df,\n\u001b[0;32m    421\u001b[0m         a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,  \u001b[38;5;66;03m# weight for retweets\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m         ticker_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m  \u001b[38;5;66;03m# extra weight for ticker symbols\u001b[39;00m\n\u001b[0;32m    427\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[8], line 216\u001b[0m, in \u001b[0;36madd_sentiment_to_df\u001b[1;34m(df, tokenizer, model)\u001b[0m\n\u001b[0;32m    213\u001b[0m         df[col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Process each row\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnalyzing sentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# Get tweet text - use preprocessed text if available\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     text \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# Analyze sentiment\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yoonus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython.display as display\n",
    "\n",
    "# Import Database class\n",
    "from Data.db_cloud import Database\n",
    "\n",
    "# Create an instance of the Database class\n",
    "db = Database()\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Define ticker mapping\n",
    "TICKER_MAPPING = {\n",
    "    1: \"XAUUSD\",\n",
    "    2: \"BTCUSD\",\n",
    "    3: \"AAPL\",\n",
    "    4: \"AMZN\",\n",
    "    5: \"NVDA\"\n",
    "}\n",
    "# Dictionary of search term weights\n",
    "SEARCH_TERM_WEIGHTS = {\n",
    "    \"AAPL OR Apple -from:Apple\": 2.5,  # 2.5x weight\n",
    "    \"Tim Cook -from:Apple\": 2,        # 2x weight\n",
    "    \"Apple earnings OR (AAPL earnings)\": 1.7, # 1.7x weight\n",
    "}\n",
    "DEFAULT_SEARCH_TERM_WEIGHT = 0.5\n",
    "min_tweet_count=100\n",
    "\n",
    "# Function to preprocess tweet text\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess tweet text by handling mentions and links\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Function to load tweets from database based on date range and ticker\n",
    "def load_tweets_from_db(start_date=None, end_date=None, ticker_id=0):\n",
    "    \"\"\"\n",
    "    Load tweets from SQLite database based on date range and ticker\n",
    "    \n",
    "    Args:\n",
    "        start_date (str, optional): Start date in YYYY-MM-DD format\n",
    "        end_date (str, optional): End date in YYYY-MM-DD format\n",
    "        ticker_id (int, optional): ID of the ticker\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with loaded tweets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to datetime objects if provided\n",
    "        start_dt = None\n",
    "        end_dt = None\n",
    "        \n",
    "        if start_date:\n",
    "            start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        \n",
    "        if end_date:\n",
    "            end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "            # Add one day to include the full end date\n",
    "            end_dt = end_dt + timedelta(days=1)\n",
    "        \n",
    "        # Format for database query\n",
    "        start_str = start_dt.strftime('%Y-%m-%d') if start_dt else None\n",
    "        end_str = end_dt.strftime('%Y-%m-%d') if end_dt else None\n",
    "        \n",
    "        # Get tweets from the database\n",
    "        tweets = db.get_tweets_by_date_and_ticker(\n",
    "            start_date=start_str,\n",
    "            end_date=end_str,\n",
    "            ticker_id=ticker_id\n",
    "        )\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(tweets)\n",
    "        \n",
    "        filter_description = \"\"\n",
    "        if start_date and end_date:\n",
    "            filter_description += f\"date range {start_date} to {end_date}\"\n",
    "        elif start_date:\n",
    "            filter_description += f\"from {start_date}\"\n",
    "        elif end_date:\n",
    "            filter_description += f\"until {end_date}\"\n",
    "            \n",
    "        if ticker_id:\n",
    "            ticker_name = TICKER_MAPPING.get(ticker_id, f\"ticker_id {ticker_id}\")\n",
    "            if filter_description:\n",
    "                filter_description += f\" with ticker {ticker_name}\"\n",
    "            else:\n",
    "                filter_description += f\"ticker {ticker_name}\"\n",
    "                \n",
    "        print(f\"Loaded [{len(df)}] tweets from database\" + (f\" for {filter_description}\" if filter_description else \"\"))\n",
    "        \n",
    "        # Convert date columns to datetime if they exist\n",
    "        for col in ['created_at', 'collected_at']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        # Apply preprocessing to tweet text\n",
    "        if 'tweet_text' in df.columns:\n",
    "            df['preprocessed_text'] = df['tweet_text'].apply(preprocess)\n",
    "        \n",
    "        return df\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tweets from database: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Setup for sentiment analysis with Twitter-RoBERTa\n",
    "def setup_twitter_roberta():\n",
    "    # Load the Twitter-RoBERTa tokenizer and model\n",
    "    model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        # Move model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(\"Using NVIDIA GPU.\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using CPU.\")\n",
    "        model = model.to(device)\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Twitter-RoBERTa model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to analyze sentiment with Twitter-RoBERTa\n",
    "def analyze_sentiment_roberta(text, tokenizer, model, max_length=512):\n",
    "    if not text or not isinstance(text, str) or not tokenizer or not model:\n",
    "        return \"neutral\", 0.0, 0.0\n",
    "    \n",
    "    try:\n",
    "        # Preprocess the text\n",
    "        preprocessed_text = preprocess(text)\n",
    "        \n",
    "        # Encode the text\n",
    "        inputs = tokenizer(preprocessed_text, return_tensors=\"pt\", max_length=max_length, \n",
    "                           truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to GPU if available        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "            \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Get prediction (0: negative, 1: neutral, 2: positive)\n",
    "        # Note: The Twitter-RoBERTa model has labels ordered as [negative, neutral, positive]\n",
    "        prediction = predictions[0].tolist()\n",
    "        sentiment_id = np.argmax(prediction)  \n",
    "        \n",
    "        labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "        sentiment_label = labels[sentiment_id]\n",
    "        \n",
    "        # Get confidence score (highest probability)\n",
    "        confidence = max(prediction)\n",
    "        \n",
    "        # Convert to score between -1 and 1\n",
    "        if sentiment_id == 0:  # negative\n",
    "            score = -prediction[0]\n",
    "        elif sentiment_id == 2:  # positive\n",
    "            score = prediction[2]\n",
    "        else:  # neutral\n",
    "            score = 0.0\n",
    "            \n",
    "        return sentiment_label, float(score), float(confidence)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return \"neutral\", 0.0, 0.0\n",
    "\n",
    "# Function to add sentiment analysis to DataFrame\n",
    "def add_sentiment_to_df(df, tokenizer, model):\n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['tweet_text']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Error: Column '{col}' not found in DataFrame\")\n",
    "            return df\n",
    "    \n",
    "    # Add new columns if they don't exist\n",
    "    for col in ['sentiment_label', 'sentiment_score', 'sentiment_confidence', 'weighted_sentiment', 'sentiment_value']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Analyzing sentiment\"):\n",
    "        # Get tweet text - use preprocessed text if available\n",
    "        text = row.get('preprocessed_text', row['tweet_text'])\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        sentiment_label, sentiment_score, confidence = analyze_sentiment_roberta(text, tokenizer, model)\n",
    "        \n",
    "        # Convert sentiment_label to numerical value (-1, 0, 1)\n",
    "        sentiment_value = -1 if sentiment_label == \"negative\" else (1 if sentiment_label == \"positive\" else 0)\n",
    "        \n",
    "        # Update DataFrame\n",
    "        df.at[idx, 'sentiment_label'] = sentiment_label\n",
    "        df.at[idx, 'sentiment_score'] = sentiment_score\n",
    "        df.at[idx, 'sentiment_confidence'] = confidence\n",
    "        df.at[idx, 'sentiment_value'] = sentiment_value\n",
    "        \n",
    "        # Calculate basic weighted sentiment (Ss * Sa)\n",
    "        df.at[idx, 'weighted_sentiment'] = sentiment_score * confidence\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to calculate engagement-weighted sentiment\n",
    "def calculate_engagement_weighted_sentiment(df, a=0.3, b=0.4, c=0.2, d=0.1, e=0.8, ticker_weight=1.5):\n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['retweet_count', 'like_count', 'reply_count', 'author_followers', 'search_term']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found. Using placeholder values.\")\n",
    "            if col == 'retweet_count':\n",
    "                df['retweet_count'] = 0\n",
    "            elif col == 'like_count':\n",
    "                df['like_count'] = 0\n",
    "            elif col == 'reply_count':\n",
    "                df['reply_count'] = 0\n",
    "            elif col == 'author_followers':\n",
    "                df['author_followers'] = 0\n",
    "            elif col == 'search_term':\n",
    "                df['search_term'] = \"\"\n",
    "    \n",
    "    # Add new columns\n",
    "    df['engagement_score'] = 0.0\n",
    "    df['final_weighted_sentiment'] = 0.0\n",
    "    \n",
    "    # Normalize follower counts (to avoid extreme values)\n",
    "    # Use log scale with a small constant to handle zeros\n",
    "    if 'author_followers' in df.columns and df['author_followers'].max() > 0:\n",
    "        df['normalized_followers'] = np.log1p(df['author_followers']) / np.log1p(df['author_followers'].max())\n",
    "    else:\n",
    "        df['normalized_followers'] = 0\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        # Skip rows with no sentiment data\n",
    "        if pd.isna(row['sentiment_score']) or pd.isna(row['sentiment_confidence']):\n",
    "            continue\n",
    "        \n",
    "        # Get engagement metrics\n",
    "        tr = row['retweet_count'] if 'retweet_count' in df.columns else 0\n",
    "        ti = row['like_count'] if 'like_count' in df.columns else 0\n",
    "        tc = row['reply_count'] if 'reply_count' in df.columns else 0\n",
    "        tf = row['normalized_followers'] \n",
    "        \n",
    "        # Calculate engagement score\n",
    "        engagement = (a * tr + b * ti + c * tc + d * tf)\n",
    "        \n",
    "        # User influence factor (using verified status and blue verification as a proxy)\n",
    "        ui = 1.0  # base influence\n",
    "        if 'author_verified' in df.columns and row['author_verified']:\n",
    "            ui += 0.3  # bonus for legacy verified accounts\n",
    "        if 'author_blue_verified' in df.columns and row['author_blue_verified']:\n",
    "            ui += 0.1  # smaller bonus for blue verified accounts\n",
    "        \n",
    "        # Apply user influence with hyperparameter e\n",
    "        user_factor = ui * e\n",
    "        \n",
    "        # Ticker detection logic\n",
    "        ticker_factor = 1.0\n",
    "        \n",
    "        # Apply ticker weight if ticker_id is present and valid\n",
    "        if 'ticker_id' in df.columns and row['ticker_id'] > 0:\n",
    "            ticker_factor = ticker_weight\n",
    "        \n",
    "        # Add search term weighting\n",
    "        search_term_factor = DEFAULT_SEARCH_TERM_WEIGHT  # Start with default weight\n",
    "        if 'search_term' in df.columns and row['search_term']:\n",
    "            # Apply custom weight if the search term matches one in our dictionary\n",
    "            if row['search_term'] in SEARCH_TERM_WEIGHTS:\n",
    "                search_term_factor = SEARCH_TERM_WEIGHTS[row['search_term']]\n",
    "                \n",
    "        # Calculate final weighted sentiment\n",
    "        base_sentiment = row['sentiment_score'] * row['sentiment_confidence']\n",
    "        final_sentiment = base_sentiment * (1 + engagement) * user_factor * ticker_factor * search_term_factor\n",
    "        \n",
    "        # Update DataFrame\n",
    "        df.at[idx, 'engagement_score'] = engagement\n",
    "        df.at[idx, 'user_influence'] = user_factor\n",
    "        df.at[idx, 'ticker_factor'] = ticker_factor\n",
    "        df.at[idx, 'search_term_factor'] = search_term_factor  \n",
    "        df.at[idx, 'final_weighted_sentiment'] = final_sentiment\n",
    "        \n",
    "    # Normalize all final sentiment values to range between -1 and 1\n",
    "    if not df['final_weighted_sentiment'].empty:\n",
    "        max_abs_sentiment = df['final_weighted_sentiment'].abs().max()\n",
    "        if max_abs_sentiment > 0:  # Avoid division by zero\n",
    "            df['final_weighted_sentiment'] = df['final_weighted_sentiment'] / max_abs_sentiment\n",
    "    \n",
    "    # Alternative normalization using sigmoid function\n",
    "    # df['final_weighted_sentiment'] = 2 * (1 / (1 + np.exp(-0.001 * df['final_weighted_sentiment']))) - 1\n",
    "    return df\n",
    "\n",
    "# Function to update tweets in the database with sentiment scores\n",
    "def update_tweets_in_db(df):\n",
    "    updated_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Updating database\"):\n",
    "        try:\n",
    "            # Get the tweet_id\n",
    "            tweet_id = row['tweet_id']\n",
    "            \n",
    "            # Update the tweet in the database\n",
    "            db.update_tweet_sentiment(\n",
    "                tweet_id=tweet_id,\n",
    "                sentiment_label=row.get('sentiment_label', ''),\n",
    "                sentiment_score=float(row.get('sentiment_score', 0.0)),\n",
    "                sentiment_magnitude=float(row.get('sentiment_confidence', 0.0)),  # Using confidence as magnitude\n",
    "                weighted_sentiment=float(row.get('final_weighted_sentiment', 0.0))\n",
    "            )\n",
    "            \n",
    "            updated_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating tweet {row.get('tweet_id', 'unknown')}: {e}\")\n",
    "    \n",
    "    print(f\"Updated {updated_count} tweets in database\")\n",
    "    return updated_count\n",
    "\n",
    "# Function to visualize sentiment analysis results - with two visualizations only\n",
    "def visualize_sentiment_analysis(df, ticker_id=None):\n",
    "    # Get ticker symbol if ticker_id is provided\n",
    "    ticker_symbol = None\n",
    "    if ticker_id is not None and ticker_id in TICKER_MAPPING:\n",
    "        ticker_symbol = TICKER_MAPPING[ticker_id]\n",
    "    \n",
    "    # Set title suffix based on ticker\n",
    "    title_suffix = f\" for {ticker_symbol}\" if ticker_symbol else \"\"\n",
    "    \n",
    "    # Set up figure with 1 row, 2 columns for the two remaining visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # 1. Sentiment distribution pie chart\n",
    "    sentiment_counts = df['sentiment_label'].value_counts()\n",
    "    \n",
    "    # Define color mapping for sentiment labels\n",
    "    color_map = {'negative': 'red', 'neutral': 'gray', 'positive': 'green'}\n",
    "    \n",
    "    # Get colors in the same order as sentiment_counts index\n",
    "    colors = [color_map[label] for label in sentiment_counts.index]\n",
    "    \n",
    "    axes[0].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
    "                colors=colors, textprops={'fontsize': 12})\n",
    "    axes[0].set_title(f'Tweet Sentiment Distribution{title_suffix}', fontsize=14)\n",
    "    \n",
    "    # 2. Histogram of sentiment scores\n",
    "    sns.histplot(df['sentiment_score'], bins=20, ax=axes[1])\n",
    "    axes[1].set_title(f'Distribution of Sentiment Scores{title_suffix}', fontsize=14)\n",
    "    axes[1].set_xlabel('Sentiment Score (-1 to 1)', fontsize=12)\n",
    "    axes[1].set_ylabel('Count', fontsize=12)\n",
    "    axes[1].tick_params(axis='both', labelsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_tweets_for_date_range(specific_date=None, start_date=None, end_date=None, ticker_id=None, min_tweet_count=100):\n",
    "    # from Data.collector import fetch_tweets_if_needed\n",
    "    \n",
    "    # Fetch tweets if needed and get date range for analysis\n",
    "    start_date, end_date, needs_fetch = fetch_tweets_if_needed(\n",
    "        specific_date=specific_date, \n",
    "        start_date=start_date, \n",
    "        end_date=end_date, \n",
    "        ticker_id=ticker_id, \n",
    "        min_tweet_count=min_tweet_count\n",
    "    )\n",
    "    \n",
    "    # Load tweets from database\n",
    "    df = load_tweets_from_db(start_date, end_date, ticker_id)\n",
    "    \n",
    "    # Check if DataFrame is not empty\n",
    "    if not df.empty:\n",
    "        # Check if we have enough tweets\n",
    "        if len(df) < min_tweet_count:\n",
    "            print(f\"Insufficient tweets in database for the requested date range.\")\n",
    "            return df\n",
    "        \n",
    "        # Setup Twitter-RoBERTa model\n",
    "        tokenizer, model = setup_twitter_roberta()\n",
    "        \n",
    "        # Check if tokenizer and model are loaded\n",
    "        if tokenizer and model:\n",
    "            # Add sentiment scores to DataFrame\n",
    "            df = add_sentiment_to_df(df, tokenizer, model)\n",
    "            \n",
    "            # Calculate engagement-weighted sentiment\n",
    "            df = calculate_engagement_weighted_sentiment(\n",
    "                df,\n",
    "                a=0.3,  # weight for retweets\n",
    "                b=0.4,  # weight for likes\n",
    "                c=0.2,  # weight for replies\n",
    "                d=0.1,  # weight for followers\n",
    "                e=0.8,  # weight for user influence\n",
    "                ticker_weight=1.5  # extra weight for ticker symbols\n",
    "            )\n",
    "            \n",
    "            # Update tweets in database if we fetched new data\n",
    "            if needs_fetch:\n",
    "                update_tweets_in_db(df)\n",
    "            \n",
    "            # Display sentiment statistics\n",
    "            print(\"\\nSentiment Distribution:\")\n",
    "            print(df['sentiment_label'].value_counts())\n",
    "            \n",
    "            print(\"\\nSentiment Score Statistics:\")\n",
    "            print(df['sentiment_score'].describe())\n",
    "            \n",
    "            print(\"\\nWeighted Sentiment Statistics:\")\n",
    "            print(df['final_weighted_sentiment'].describe())\n",
    "            \n",
    "            # Visualize results\n",
    "            visualize_sentiment_analysis(df, ticker_id)\n",
    "            \n",
    "            # Additional analysis: Most influential tweets\n",
    "            if 'final_weighted_sentiment' in df.columns:\n",
    "                print(\"\\nTop 5 Most Positive Influential Tweets:\")\n",
    "                top_positive = df.sort_values('final_weighted_sentiment', ascending=False).head(5)\n",
    "                for idx, row in top_positive.iterrows():\n",
    "                    print(f\"Score: {row['final_weighted_sentiment']:.4f} | {row['tweet_text']}\")\n",
    "                \n",
    "                print(\"\\nTop 5 Most Negative Influential Tweets:\")\n",
    "                top_negative = df.sort_values('final_weighted_sentiment').head(5)\n",
    "                for idx, row in top_negative.iterrows():\n",
    "                    print(f\"Score: {row['final_weighted_sentiment']:.4f} | {row['tweet_text']}\")\n",
    "                    \n",
    "            return df\n",
    "        else:\n",
    "            print(\"Error: Failed to load Twitter-RoBERTa model\")\n",
    "            return df\n",
    "    else:\n",
    "        print(\"Error: No tweets loaded from database\")\n",
    "        return df\n",
    "\n",
    "def get_tweets_sentiment_analysis(ticker_id=None, ticker_symbol=None, specific_date=None, start_date=None, end_date=None):\n",
    "\n",
    "    # Handle ticker symbol conversion to ticker_id if needed\n",
    "    if ticker_id is None and ticker_symbol:\n",
    "        # Convert symbol to ID using reverse lookup\n",
    "        ticker_id = next((id for id, symbol in TICKER_MAPPING.items() if symbol == ticker_symbol), 0)\n",
    "    \n",
    "    # Handle date parameters\n",
    "    if specific_date:\n",
    "        start_date = specific_date\n",
    "        end_date = specific_date\n",
    "    \n",
    "    # Analyze or load the data\n",
    "    df = analyze_tweets_for_date_range(\n",
    "        specific_date=specific_date,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        ticker_id=ticker_id\n",
    "    )\n",
    "    \n",
    "    # If dataframe is empty or less than min_tweet_count, return zeros/defaults\n",
    "    if df.empty or len(df) < min_tweet_count:\n",
    "        return {\n",
    "            'tweets_sentiment_score': 0.0,\n",
    "            'tweets_count': 0,\n",
    "            'most_positive_tweet': \"\",\n",
    "            'most_negative_tweet': \"\",\n",
    "            'tweets_weighted_sentiment_score': 0.0,\n",
    "        }\n",
    "    \n",
    "    # Calculate the metrics\n",
    "    # 1. Average raw sentiment score\n",
    "    avg_sentiment = df['sentiment_score'].mean()\n",
    "    \n",
    "    # 2. Tweet count\n",
    "    tweet_count = len(df)\n",
    "    \n",
    "    # 3. Find most positive tweet\n",
    "    most_positive_idx = df['final_weighted_sentiment'].idxmax()\n",
    "    most_positive_tweet = df.loc[most_positive_idx, 'tweet_text']\n",
    "    \n",
    "    # 4. Find most negative tweet\n",
    "    most_negative_idx = df['final_weighted_sentiment'].idxmin()\n",
    "    most_negative_tweet = df.loc[most_negative_idx, 'tweet_text']\n",
    "    \n",
    "    # 5. Average weighted sentiment\n",
    "    avg_weighted_sentiment = df['final_weighted_sentiment'].mean()\n",
    "    \n",
    "    # Return the metrics\n",
    "    return {\n",
    "        'tweets_sentiment_score': float(avg_sentiment),\n",
    "        'tweets_count': int(tweet_count),\n",
    "        'most_positive_tweet': str(most_positive_tweet),\n",
    "        'most_negative_tweet': str(most_negative_tweet),\n",
    "        'tweets_weighted_sentiment_score': float(avg_weighted_sentiment),\n",
    "    }\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters\n",
    "    specific_date = '2025-01-18'  # Use specific_date for a single day\n",
    "    # Alternative: use date range\n",
    "    # start_date = '2025-01-01' \n",
    "    # end_date = '2025-01-31'\n",
    "    ticker_id = 3  # AAPL\n",
    "    summary = get_tweets_sentiment_analysis(ticker_id=ticker_id, specific_date=specific_date)\n",
    "    print(\"\\n\\n============ Sentiment Summary ===========\")\n",
    "    print(summary)\n",
    "    # Analyze tweets\n",
    "    # df = analyze_tweets_for_date_range(specific_date=specific_date, ticker_id=ticker_id)\n",
    "    # Alternative: df = analyze_tweets_for_date_range(start_date=start_date, end_date=end_date, ticker_id=ticker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662ca1239e108f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# What I will do is the following:\n",
    "# Basic Scoring\n",
    "# Sentiment label: Positive, Neutral, Negative (1, 0, -1)\n",
    "# Sentiment score (Ss): A value between -1 and 1\n",
    "# Model accuracy/confidence (Sa): A value between 0 and 1 (Can be calculated using the model's confidence score) \n",
    "# Then, to get the final score, I will use the following formula:\n",
    "# Ss * Sa \n",
    "\n",
    "# I will also use a Weighted Sentiment Calculation for the Tweet-Level Calculations:\n",
    "# Tr = retweet count\n",
    "# Ti = like count\n",
    "# Tc = comment count\n",
    "# Tf = follower count\n",
    "# a, b, c, d = weights for retweet, like, comment, and follow counts (Hyperparameters) \n",
    "\n",
    "# I will also take into consideration the user influence:\n",
    "# Ui = user influence * E (Hyperparameter) (Personally Placed) 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
