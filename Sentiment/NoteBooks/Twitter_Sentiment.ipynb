{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206457c832f57fdd",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a47ad693e53c7aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T01:18:30.555410Z",
     "start_time": "2025-04-21T01:18:30.548059Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Value      \n",
      "          mean count\n",
      "Category            \n",
      "A         15.0     2\n",
      "B         40.0     2\n",
      "C         40.0     1\n"
     ]
    }
   ],
   "source": [
    "# What I will do is the following:\n",
    "# Basic Scoring\n",
    "# Sentiment label: Positive, Neutral, Negative (1, 0, -1)\n",
    "# Sentiment score (Ss): A value between -1 and 1\n",
    "# Model accuracy/confidence (Sa): A value between 0 and 1 (Can be calculated using the model's confidence score) \n",
    "# Then, to get the final score, I will use the following formula:\n",
    "# Ss * Sa \n",
    "\n",
    "# I will also use a Weighted Sentiment Calculation for the Tweet-Level Calculations:\n",
    "# Tr = retweet count\n",
    "# Ti = like count\n",
    "# Tc = comment count\n",
    "# Tf = follower count\n",
    "# a, b, c, d = weights for retweet, like, comment, and follow counts (Hyperparameters) \n",
    "\n",
    "# I will also take into consideration the user influence:\n",
    "# Ui = user influence * E (Hyperparameter) (Personally Placed) 0.8\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Category': ['A', 'A', 'B', 'C', 'B'],\n",
    "    'Value': [10, 20, 30, 40, 50]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by 'Category' and calculate mean and count\n",
    "result = df.groupby('Category').agg({\n",
    "    'Value': ['mean', 'count']\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0cc4e36011f15",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f603c83dd4926baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T19:39:29.110170Z",
     "start_time": "2025-04-14T19:39:22.969305Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database: ../Data/data.db\n",
      "Using database at: C:\\Users\\kemoo\\PycharmProjects\\Stock_AI_Predictor\\Data\\data.db\n",
      "APIFY_API_TOKEN: apify_api_TrvSNoUgTUBDan9KL95e8iszphaLNY2A1FJG\n",
      "\n",
      "======= Fetching month 1 of 1 (2025-03-15 to 2025-04-14) =======\n",
      "Fetching tweets for term: $AAPL OR Apple -from:Apple\n",
      "Fetching tweets for search term: $AAPL OR Apple -from:Apple\n",
      "Error fetching tweets for term '$AAPL OR Apple -from:Apple': Monthly usage hard limit exceeded\n",
      "No tweets found for term '$AAPL OR Apple -from:Apple' for period 2025-03-15 to 2025-04-14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 259\u001b[0m\n\u001b[0;32m    242\u001b[0m APPLE_SEARCH_TERMS \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$AAPL OR Apple -from:Apple\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    244\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTim Cook\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirPods OR (Apple headphones)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m ]\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Execute the data collection - adjust parameters as needed\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m total_tweets \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_and_store_tweets_by_period\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_terms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAPPLE_SEARCH_TERMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonths_back\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Collect 1 year of data\u001b[39;49;00m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# 500 tweets per month per term\u001b[39;49;00m\n\u001b[0;32m    263\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData collection complete. Total tweets collected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tweets\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 236\u001b[0m, in \u001b[0;36mfetch_and_store_tweets_by_period\u001b[1;34m(search_terms, months_back, batch_size)\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tweets found for term \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for period \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# Be nice to the API\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### Tweet fetching and storage completed. Total tweets stored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_stored\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ###\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_stored\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from apify_client import ApifyClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from Data.db import Database\n",
    "\n",
    "# Initialize database connection\n",
    "db = Database()\n",
    "\n",
    "# Load environment variables (for API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Token from environment variable\n",
    "APIFY_API_TOKEN = os.getenv(\"APIFY_API_TOKEN\")\n",
    "print(f\"APIFY_API_TOKEN: {APIFY_API_TOKEN}\")\n",
    "# Initialize the ApifyClient with your API token\n",
    "client = ApifyClient(APIFY_API_TOKEN)\n",
    "\n",
    "def fetch_tweets(search_terms, start_date, end_date, max_items=10, tweet_language=\"en\", \n",
    "                min_retweets=0, min_favorites=0, only_verified=False):\n",
    "    \"\"\"\n",
    "    Fetch tweets using the Twitter API through Apify\n",
    "    \n",
    "    Args:\n",
    "        search_terms (list): List of search terms to query\n",
    "        start_date (str): Start date in YYYY-MM-DD format\n",
    "        end_date (str): End date in YYYY-MM-DD format\n",
    "        max_items (int): Maximum number of tweets to retrieve per term\n",
    "        tweet_language (str): Language of tweets to retrieve\n",
    "        min_retweets (int): Minimum number of retweets\n",
    "        min_favorites (int): Minimum number of favorites/likes\n",
    "        only_verified (bool): Only include tweets from verified users\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tweet objects\n",
    "    \"\"\"\n",
    "    all_tweets = []\n",
    "    \n",
    "    for term in search_terms:\n",
    "        print(f\"Fetching tweets for search term: {term}\")\n",
    "        \n",
    "        # Prepare the Actor input\n",
    "        run_input = {\n",
    "            \"searchTerms\": [term],\n",
    "            \"tweetLanguage\": tweet_language,\n",
    "            \"minimumRetweets\": min_retweets,\n",
    "            \"minimumFavorites\": min_favorites,\n",
    "            \"start\": start_date,\n",
    "            \"end\": end_date,\n",
    "            \"maxItems\": max_items,\n",
    "            \"includeSearchTerms\": True,\n",
    "            \"sort\": \"Latest\"\n",
    "        }\n",
    "        \n",
    "        if only_verified:\n",
    "            run_input[\"filter\"] = \"verified\"\n",
    "        \n",
    "        try:\n",
    "            # Run the Actor and wait for it to finish\n",
    "            run = client.actor(\"61RPP7dywgiy0JPD0\").call(run_input=run_input)\n",
    "            # Print the run details or error message\n",
    "            print(f\"Run details: {run}\")\n",
    "            # Fetch the Actor results\n",
    "            for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "                # Add the search term to each tweet\n",
    "                item['search_term'] = term\n",
    "                all_tweets.append(item)\n",
    "                \n",
    "            print(f\"Retrieved {len(all_tweets)} tweets for term: {term}\")\n",
    "            \n",
    "            # Respect API rate limits\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching tweets for term '{term}': {e}\")\n",
    "    \n",
    "    return all_tweets\n",
    "\n",
    "def process_tweet(tweet, search_term):\n",
    "    \"\"\"\n",
    "    Process a single tweet and extract relevant fields without sentiment analysis\n",
    "    \n",
    "    Args:\n",
    "        tweet (dict): Tweet object from the API\n",
    "        search_term (str): The search term that found this tweet\n",
    "        \n",
    "    Returns:\n",
    "        dict: Processed tweet data\n",
    "    \"\"\"\n",
    "    # Current timestamp\n",
    "    current_time = datetime.now().isoformat()\n",
    "    \n",
    "    # Extract empty placeholders for sentiment fields (to be filled later)\n",
    "    sentiment_label = \"\"\n",
    "    sentiment_score = 0.0\n",
    "    sentiment_magnitude = 0.0\n",
    "    weighted_sentiment = 0.0\n",
    "    \n",
    "    # Return processed tweet data\n",
    "    return {\n",
    "        'tweet_id': tweet.get('id', ''),\n",
    "        'tweet_text': tweet.get('text', ''),\n",
    "        'created_at': tweet.get('createdAt', ''),\n",
    "        'retweet_count': tweet.get('retweetCount', 0),\n",
    "        'reply_count': tweet.get('replyCount', 0),\n",
    "        'like_count': tweet.get('likeCount', 0),\n",
    "        'quote_count': tweet.get('quoteCount', 0),\n",
    "        'bookmark_count': tweet.get('bookmarkCount', 0),\n",
    "        'lang': tweet.get('lang', ''),\n",
    "        'is_reply': tweet.get('isReply', False),\n",
    "        'is_quote': tweet.get('isQuote', False),\n",
    "        'is_retweet': tweet.get('isRetweet', False),\n",
    "        'url': tweet.get('url', ''),\n",
    "        'search_term': search_term,\n",
    "        'author_username': tweet.get('author', {}).get('userName', ''),\n",
    "        'author_name': tweet.get('author', {}).get('name', ''),\n",
    "        'author_verified': tweet.get('author', {}).get('isVerified', False),\n",
    "        'author_blue_verified': tweet.get('author', {}).get('isBlueVerified', False),\n",
    "        'author_followers': tweet.get('author', {}).get('followers', 0),\n",
    "        'author_following': tweet.get('author', {}).get('following', 0),\n",
    "        'sentiment_label': sentiment_label,\n",
    "        'sentiment_score': sentiment_score,\n",
    "        'sentiment_magnitude': sentiment_magnitude,\n",
    "        'weighted_sentiment': weighted_sentiment,\n",
    "        'collected_at': current_time\n",
    "    }\n",
    "\n",
    "def store_tweets_in_db(tweets):\n",
    "    \"\"\"\n",
    "    Store processed tweets in the database\n",
    "    \n",
    "    Args:\n",
    "        tweets (list): List of processed tweet dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tweets stored\n",
    "    \"\"\"\n",
    "    stored_count = 0\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            db.store_tweets(\n",
    "                tweet_id=tweet['tweet_id'],\n",
    "                tweet_text=tweet['tweet_text'],\n",
    "                created_at=tweet['created_at'],\n",
    "                retweet_count=tweet['retweet_count'],\n",
    "                reply_count=tweet['reply_count'],\n",
    "                like_count=tweet['like_count'],\n",
    "                quote_count=tweet['quote_count'],\n",
    "                bookmark_count=tweet['bookmark_count'],\n",
    "                lang=tweet['lang'],\n",
    "                is_reply=tweet['is_reply'],\n",
    "                is_quote=tweet['is_quote'],\n",
    "                is_retweet=tweet['is_retweet'],\n",
    "                url=tweet['url'],\n",
    "                search_term=tweet['search_term'],\n",
    "                author_username=tweet['author_username'],\n",
    "                author_name=tweet['author_name'],\n",
    "                author_verified=tweet['author_verified'],\n",
    "                author_blue_verified=tweet['author_blue_verified'],\n",
    "                author_followers=tweet['author_followers'],\n",
    "                author_following=tweet['author_following'],\n",
    "                sentiment_label=tweet['sentiment_label'],\n",
    "                sentiment_score=tweet['sentiment_score'],\n",
    "                sentiment_magnitude=tweet['sentiment_magnitude'],\n",
    "                weighted_sentiment=tweet['weighted_sentiment'],\n",
    "                collected_at=tweet['collected_at']\n",
    "            )\n",
    "            stored_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error storing tweet {tweet['tweet_id']}: {e}\")\n",
    "    \n",
    "    return stored_count\n",
    "\n",
    "def fetch_and_store_tweets_by_period(search_terms, months_back=12, batch_size=10):\n",
    "    \"\"\"\n",
    "    Fetch tweets for multiple search terms in monthly chunks and store in DB\n",
    "    \n",
    "    Args:\n",
    "        search_terms (list): List of search terms\n",
    "        months_back (int): Number of months to look back\n",
    "        batch_size (int): Number of tweets per batch\n",
    "        \n",
    "    Returns:\n",
    "        int: Total number of tweets stored\n",
    "    \"\"\"\n",
    "    total_stored = 0\n",
    "    \n",
    "    # Calculate date ranges for each month\n",
    "    end_date = datetime.now()\n",
    "    \n",
    "    # Process each month\n",
    "    for month in range(months_back):\n",
    "        month_end = end_date - timedelta(days=30 * month)\n",
    "        month_start = end_date - timedelta(days=30 * (month + 1))\n",
    "        \n",
    "        end_str = month_end.strftime('%Y-%m-%d')\n",
    "        start_str = month_start.strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"\\n======= Fetching month {month+1} of {months_back} ({start_str} to {end_str}) =======\")\n",
    "        \n",
    "        # Process each search term\n",
    "        for term in search_terms:\n",
    "            print(f\"Fetching tweets for term: {term}\")\n",
    "            \n",
    "            # Fetch tweets for this term and date range\n",
    "            term_tweets = fetch_tweets(\n",
    "                search_terms=[term],\n",
    "                start_date=start_str,\n",
    "                end_date=end_str,\n",
    "                max_items=batch_size,\n",
    "                tweet_language=\"en\",\n",
    "                min_retweets=10,\n",
    "                min_favorites=25,\n",
    "                only_verified=True\n",
    "            )\n",
    "            \n",
    "            if term_tweets:\n",
    "                # Process tweets\n",
    "                processed_tweets = [process_tweet(tweet, term) for tweet in term_tweets]\n",
    "                \n",
    "                # Store in database\n",
    "                stored_count = store_tweets_in_db(processed_tweets)\n",
    "                total_stored += stored_count\n",
    "                \n",
    "                print(f\"Stored {stored_count} tweets for term '{term}' for period {start_str} to {end_str}\")\n",
    "            else:\n",
    "                print(f\"No tweets found for term '{term}' for period {start_str} to {end_str}\")\n",
    "            \n",
    "            # Be nice to the API\n",
    "            time.sleep(5)\n",
    "    \n",
    "    print(f\"### Tweet fetching and storage completed. Total tweets stored: {total_stored} ###\")\n",
    "    return total_stored\n",
    "\n",
    "# Define search terms for Apple\n",
    "APPLE_SEARCH_TERMS = [\n",
    "    \"$AAPL OR Apple -from:Apple\", \n",
    "    \"Tim Cook\", \n",
    "    \"iPhone\", \n",
    "    \"WWDC OR (Apple event)\",\n",
    "    \"MacBook OR Macbook OR (Mac Pro) OR iMac\",\n",
    "    \"iPad OR iPadOS\",\n",
    "    \"iOS OR iPadOS OR macOS\",\n",
    "    \"Apple earnings OR (AAPL earnings)\",\n",
    "    \"(Apple stock) OR (AAPL stock) OR (Apple shares)\",\n",
    "    \"Apple AI OR (Apple intelligence)\",\n",
    "    \"Apple Vision Pro\",\n",
    "    \"Apple Watch\",\n",
    "    \"AirPods OR (Apple headphones)\"\n",
    "]\n",
    "\n",
    "# Execute the data collection - adjust parameters as needed\n",
    "total_tweets = fetch_and_store_tweets_by_period(\n",
    "    search_terms=APPLE_SEARCH_TERMS,\n",
    "    months_back=1,  # Collect 1 year of data\n",
    "    batch_size=5   # 500 tweets per month per term\n",
    ")\n",
    "\n",
    "print(f\"Data collection complete. Total tweets collected: {total_tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1229edd2a9d4a9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
