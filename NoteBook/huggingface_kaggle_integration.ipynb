{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b4eb1ae",
   "metadata": {},
   "source": [
    "# Stock AI Predictor - Automated Update and Retraining\n",
    "\n",
    "This Kaggle notebook automates the updating of datasets and retraining of models for the Stock AI Predictor project on Hugging Face. It can be scheduled to run at regular intervals using Kaggle's scheduling feature.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The workflow performs the following tasks:\n",
    "\n",
    "1. Fetches the latest market data\n",
    "2. Updates the dataset on Hugging Face Datasets\n",
    "3. Retrains the parameter tester model if needed\n",
    "4. Retrains the RL trading model if needed\n",
    "5. Deploys updated models to Hugging Face Hub\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf907f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install huggingface_hub datasets pandas numpy requests python-dotenv stable-baselines3 gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc7291",
   "metadata": {},
   "source": [
    "## Clone Project Repository\n",
    "\n",
    "Now, let's clone the project repository from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/yourusername/Stock_AI_Predictor.git\n",
    "%cd Stock_AI_Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ffafc",
   "metadata": {},
   "source": [
    "## Set Up Environment Variables\n",
    "\n",
    "We need to set up the environment variables for Hugging Face API access. In Kaggle, you should add these as secrets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10171bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment variables from Kaggle secrets\n",
    "# Note: You need to add these secrets to your Kaggle notebook settings\n",
    "os.environ[\"HF_TOKEN\"] = \"your_huggingface_token\"  # Replace with actual token or use Kaggle secrets\n",
    "os.environ[\"PARAM_TESTER_REPO_ID\"] = \"your_username/stock-ai-parameter-tester\"\n",
    "os.environ[\"RL_MODEL_REPO_ID\"] = \"your_username/stock-ai-rl-trader\"\n",
    "os.environ[\"DATASET_REPO_ID\"] = \"your_username/stock-market-data\"\n",
    "os.environ[\"API_SPACE_ID\"] = \"your_username/stock-ai-predictor-api\"\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2be7eb",
   "metadata": {},
   "source": [
    "## Data Fetching\n",
    "\n",
    "Now, let's implement the data fetching part that will update our database with the latest market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e214e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from Data.Database.db import Database\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_market_data():\n",
    "    \"\"\"Fetch latest market data and update database\"\"\"\n",
    "    print(\"Fetching latest market data...\")\n",
    "    \n",
    "    # Initialize database\n",
    "    db = Database()\n",
    "    \n",
    "    # Get list of stocks/symbols to update\n",
    "    stocks_query = \"SELECT id, symbol FROM stocks\"\n",
    "    stocks_df = pd.read_sql_query(stocks_query, db.connection)\n",
    "    \n",
    "    # Get list of timeframes\n",
    "    timeframes_query = \"SELECT id, name FROM timeframes\"\n",
    "    timeframes_df = pd.read_sql_query(timeframes_query, db.connection)\n",
    "    \n",
    "    # For each stock and timeframe, fetch and update data\n",
    "    for _, stock_row in stocks_df.iterrows():\n",
    "        stock_id = stock_row['id']\n",
    "        symbol = stock_row['symbol']\n",
    "        \n",
    "        for _, tf_row in timeframes_df.iterrows():\n",
    "            tf_id = tf_row['id']\n",
    "            tf_name = tf_row['name']\n",
    "            \n",
    "            print(f\"Updating {symbol} for timeframe {tf_name}\")\n",
    "            \n",
    "            # Fetch latest data for this stock and timeframe\n",
    "            # This is where you would implement your data fetching logic\n",
    "            # For example, using yfinance, alpha_vantage, or another API\n",
    "            \n",
    "            # Example using a dummy function (you'd replace this with actual data fetching)\n",
    "            new_data = fetch_latest_prices(symbol, tf_name)\n",
    "            \n",
    "            # Update database with new data\n",
    "            update_database(db, stock_id, tf_id, new_data)\n",
    "    \n",
    "    print(\"Market data update completed\")\n",
    "\n",
    "def fetch_latest_prices(symbol, timeframe):\n",
    "    \"\"\"Dummy function to fetch latest prices (replace with actual implementation)\"\"\"\n",
    "    # In a real implementation, you would use yfinance, alpha_vantage, etc.\n",
    "    # For demonstration purposes, we'll create some dummy data\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    \n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    prices = np.random.normal(100, 5, size=len(dates))\n",
    "    volumes = np.random.randint(1000, 10000, size=len(dates))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'datetime': dates,\n",
    "        'open': prices,\n",
    "        'high': prices * 1.02,\n",
    "        'low': prices * 0.98,\n",
    "        'close': prices * (1 + np.random.normal(0, 0.01, size=len(dates))),\n",
    "        'volume': volumes\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "def update_database(db, stock_id, timeframe_id, data):\n",
    "    \"\"\"Update database with new data (implement the actual logic)\"\"\"\n",
    "    # This is where you would implement your database update logic\n",
    "    # For example, inserting new rows, updating existing ones, etc.\n",
    "    print(f\"Updated database for stock_id={stock_id}, timeframe_id={timeframe_id} with {len(data)} records\")\n",
    "\n",
    "# Run data fetching\n",
    "fetch_market_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc8b60",
   "metadata": {},
   "source": [
    "## Dataset Deployment\n",
    "\n",
    "Now, let's prepare and upload the dataset to Hugging Face Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deployment.dataset_uploader import prepare_dataset, upload_dataset\n",
    "import os\n",
    "\n",
    "def deploy_dataset():\n",
    "    \"\"\"Prepare and upload dataset to Hugging Face Hub\"\"\"\n",
    "    print(\"Deploying dataset to Hugging Face Datasets...\")\n",
    "    \n",
    "    # Get repository ID from environment\n",
    "    dataset_repo_id = os.environ.get(\"DATASET_REPO_ID\")\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    \n",
    "    if not dataset_repo_id or not hf_token:\n",
    "        raise ValueError(\"Dataset repository ID and HF token must be provided\")\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset = prepare_dataset()\n",
    "    \n",
    "    # Upload dataset\n",
    "    upload_dataset(dataset, repo_id=dataset_repo_id, token=hf_token)\n",
    "    \n",
    "    print(f\"Dataset deployed to {dataset_repo_id}\")\n",
    "\n",
    "# Deploy dataset\n",
    "deploy_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b819b9",
   "metadata": {},
   "source": [
    "## Model Retraining and Deployment\n",
    "\n",
    "Now, let's retrain the models if needed and deploy them to Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3a3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deployment.deploy_to_huggingface import HuggingFaceDeployer\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def should_retrain_models():\n",
    "    \"\"\"Determine if models should be retrained\"\"\"\n",
    "    # For example, retrain on the 1st and 15th of each month\n",
    "    today = datetime.now()\n",
    "    return today.day in [1, 15]\n",
    "\n",
    "def retrain_and_deploy_models():\n",
    "    \"\"\"Retrain models if needed and deploy to Hugging Face Hub\"\"\"\n",
    "    if not should_retrain_models():\n",
    "        print(\"Model retraining not scheduled for today. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Retraining and deploying models to Hugging Face Hub...\")\n",
    "    \n",
    "    # Get repository IDs from environment\n",
    "    param_tester_repo_id = os.environ.get(\"PARAM_TESTER_REPO_ID\")\n",
    "    rl_model_repo_id = os.environ.get(\"RL_MODEL_REPO_ID\")\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    \n",
    "    if not param_tester_repo_id or not rl_model_repo_id or not hf_token:\n",
    "        raise ValueError(\"Repository IDs and HF token must be provided\")\n",
    "    \n",
    "    # Initialize deployer\n",
    "    deployer = HuggingFaceDeployer(token=hf_token)\n",
    "    \n",
    "    # Retrain parameter tester model\n",
    "    print(\"Retraining parameter tester model...\")\n",
    "    # This is where you would implement your parameter tester retraining logic\n",
    "    # For example: from Colab.parameter_tester import ParameterTester\n",
    "    # tester = ParameterTester()\n",
    "    # tester.run_optimization()\n",
    "    \n",
    "    # Deploy parameter tester model\n",
    "    deployer.deploy_parameter_tester(repo_id=param_tester_repo_id)\n",
    "    \n",
    "    # Retrain RL model\n",
    "    print(\"Retraining RL trading model...\")\n",
    "    # This is where you would implement your RL model retraining logic\n",
    "    # For example: from RL.Scripts.train_rl_model import train_model\n",
    "    # train_model(epochs=100)\n",
    "    \n",
    "    # Deploy RL model\n",
    "    deployer.deploy_rl_model(repo_id=rl_model_repo_id)\n",
    "    \n",
    "    print(\"Models retrained and deployed successfully\")\n",
    "\n",
    "# Retrain and deploy models\n",
    "retrain_and_deploy_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8b27f",
   "metadata": {},
   "source": [
    "## Summary and Verification\n",
    "\n",
    "Let's summarize what we've done and verify that everything was updated correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced41589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "def verify_deployment():\n",
    "    \"\"\"Verify that all components were deployed correctly\"\"\"\n",
    "    print(\"Verifying deployment...\")\n",
    "    \n",
    "    # Get repository IDs from environment\n",
    "    param_tester_repo_id = os.environ.get(\"PARAM_TESTER_REPO_ID\")\n",
    "    rl_model_repo_id = os.environ.get(\"RL_MODEL_REPO_ID\")\n",
    "    dataset_repo_id = os.environ.get(\"DATASET_REPO_ID\")\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    \n",
    "    if not any([param_tester_repo_id, rl_model_repo_id, dataset_repo_id, hf_token]):\n",
    "        print(\"Repository IDs and HF token must be provided for verification\")\n",
    "        return\n",
    "    \n",
    "    # Initialize Hugging Face API\n",
    "    api = HfApi(token=hf_token)\n",
    "    \n",
    "    # Check parameter tester model\n",
    "    if param_tester_repo_id:\n",
    "        try:\n",
    "            model_info = api.model_info(param_tester_repo_id)\n",
    "            print(f\"Parameter tester model verified: {param_tester_repo_id}\")\n",
    "            print(f\"  Last updated: {model_info.siblings[-1].lastModified if model_info.siblings else 'N/A'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Parameter tester model verification failed: {e}\")\n",
    "    \n",
    "    # Check RL model\n",
    "    if rl_model_repo_id:\n",
    "        try:\n",
    "            model_info = api.model_info(rl_model_repo_id)\n",
    "            print(f\"RL trading model verified: {rl_model_repo_id}\")\n",
    "            print(f\"  Last updated: {model_info.siblings[-1].lastModified if model_info.siblings else 'N/A'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"RL trading model verification failed: {e}\")\n",
    "    \n",
    "    # Check dataset\n",
    "    if dataset_repo_id:\n",
    "        try:\n",
    "            dataset_info = api.dataset_info(dataset_repo_id)\n",
    "            print(f\"Dataset verified: {dataset_repo_id}\")\n",
    "            print(f\"  Last updated: {dataset_info.lastModified}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Dataset verification failed: {e}\")\n",
    "    \n",
    "    print(\"Verification completed\")\n",
    "\n",
    "# Verify deployment\n",
    "verify_deployment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c910ae5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "1. Fetched the latest market data\n",
    "2. Updated the dataset on Hugging Face Datasets\n",
    "3. Retrained and deployed models to Hugging Face Hub (if scheduled)\n",
    "\n",
    "You can schedule this notebook to run weekly on Kaggle to keep your dataset and models up-to-date.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Set up Kaggle scheduling for this notebook\n",
    "2. Configure the actual data fetching logic for your specific data sources\n",
    "3. Implement the actual model training logic based on your project requirements"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
